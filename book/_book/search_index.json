[["examples-of-estimating-cross-lagged-panel-models-in-r.html", "Section 2 Examples of Estimating Cross-Lagged Panel Models in R 2.1 Simulation 2.2 A Simple Example 2.3 Summary: Change versus Level Scores 2.4 Unobserved Heterogeneity 2.5 Model Validation 2.6 Short t bias. 2.7 The Fixed Effect (Dynamic Panel) Estimator 2.8 The RICLPM 2.9 Change 2.10 Summary", " Section 2 Examples of Estimating Cross-Lagged Panel Models in R The package that accompanies this summary, crossLagR, includes functions to estimate several panel data models. They’re structured such that functions that begin with estimate are simple functions to write latent variable panel models. The (working) package can be accessed on github: devtools::install_github(\"crweber9874/crossLagR\") Below I use four functions, with simulated data, which is expanded to real panel data in the next section. estimateCLPM() generates the syntax for cross-lagged panel models. Use help(estimateRICLPM) to modify constraints and number of waves. estimateRICLPM() generates the syntax for random-intercept cross-lagged panel models. Use help(estimateRICLPM). estimateLChange() generates the syntax for latent change score model. Use help(estimateLChange). estimateLGM() generates the syntax for latent growth models. Use help(estimateLGM). Nearly all the functions in the package start with a core lavaan model – such as the RICLPM (Hamaker, Kuiper, and Grasman 2015). From there, build in rules to implement constraints, extend the number of waves, and modify parameters. You could just print the model code and modify manually. For instance, the lavaan code to estimate the CLPM is, estimateCLPM(waves = 2, constrain_beta = TRUE, constrain_omega = TRUE, constrain_residual_variances = TRUE, constrain_residual_covariances = TRUE, estimate_means = FALSE) |&gt; cat() ## p1 =~ 1*y1 ## q1 =~ 1*x1 ## p2 =~ 1*y2 ## q2 =~ 1*x2 ## p1 ~ 0*1 ## q1 ~ 0*1 ## p2 ~ 0*1 ## q2 ~ 0*1 ## y1 ~ 0*1 ## x1 ~0*1 ## y2 ~ 0*1 ## x2 ~0*1 ## p2 ~ beta_y*p1 + omega_xy*q1 ## q2 ~ beta_x*q1 + omega_yx*p1 ## p1 ~~ var_y1*p1 ## q1 ~~ var_x1*q1 ## p2 ~~ var_y*p2 ## q2 ~~ var_x*q2 ## p1 ~~ cov_xy1*q1 ## p2 ~~ cov_xy*q2 ## y1 ~~ 0*y1 ## x1 ~~ 0*x1 ## y2 ~~ 0*y2 ## x2 ~~ 0*x2 By modifying constrain_beta and constrain_omega, this frees the AR (former) or CL (latter) parameters to vary across waves. The lavaan code to estimate the Bivariate Latent Change Model is, estimateLChange(waves = 3, variable_type = &quot;bivariate&quot;, constrain_omega = TRUE, constrain_beta = TRUE) |&gt; cat() ## cf_x1 =~ 1*x1 ## cf_x2 =~ 1*x2 ## cf_x3 =~ 1*x3 ## cf_x1 ~ indicator_mean_x*1 ## cf_x2 ~ 0*1 ## cf_x3 ~ 0*1 ## cf_x1 ~~ start(15)*latent_indicator_mean_x*cf_x1 ## cf_x2 ~~ 0*cf_x2 ## cf_x3 ~~ 0*cf_x3 ## x1 ~ 0*1 ## x2 ~ 0*1 ## x3 ~ 0*1 ## x1 ~~ indicator_variance_x*x1 ## x2 ~~ indicator_variance_x*x2 ## x3 ~~ indicator_variance_x*x3 ## cf_x2 ~ 1*cf_x1 ## cf_x3 ~ 1*cf_x2 ## ld_x2 =~ 1*cf_x2 ## ld_x3 =~ 1*cf_x3 ## ld_x2 ~ 0*1 ## ld_x3 ~ 0*1 ## ld_x2 ~~ 0*ld_x2 ## ld_x3 ~~ 0*ld_x3 ## general_x =~ 1*ld_x2 ## + 1*ld_x3 ## general_x ~ constant_mean_x*1 ## general_x ~~ latent_variance_x*general_x ## general_x ~~ cf_x1 ## ld_x2 ~ start(-0.15)*beta_x*cf_x1 ## ld_x3 ~ start(-0.15)*beta_x*cf_x2 ## cf_y1 =~ 1*y1 ## cf_y2 =~ 1*y2 ## cf_y3 =~ 1*y3 ## cf_y1 ~ indicator_mean_y*1 ## cf_y2 ~ 0*1 ## cf_y3 ~ 0*1 ## cf_y1 ~~ start(15)*latent_variance_y*cf_y1 ## cf_y2 ~~ 0*cf_y2 ## cf_y3 ~~ 0*cf_y3 ## y1 ~ 0*1 ## y2 ~ 0*1 ## y3 ~ 0*1 ## y1 ~~ indicator_variance_y*y1 ## y2 ~~ indicator_variance_y*y2 ## y3 ~~ indicator_variance_y*y3 ## cf_y2 ~ 1*cf_y1 ## cf_y3 ~ 1*cf_y2 ## ld_y2 =~ 1*cf_y2 ## ld_y3 =~ 1*cf_y3 ## ld_y2 ~ 0*1 ## ld_y3 ~ 0*1 ## ld_y2 ~~ 0*ld_y2 ## ld_y3 ~~ 0*ld_y3 ## general_y =~ 1*ld_y2 ## + 1*ld_y3 ## general_y ~ constant_mean_y*1 ## general_y ~~ latent_variance_y*general_y ## general_y ~~ cf_y1 ## ld_y2 ~ beta_y*cf_y1 ## ld_y3 ~ beta_y*cf_y2 ## ld_x2 ~ start(-0.2)*omega_x*cf_y1 ## ld_y2 ~ start(-0.2)*omega_y*cf_x1 ## ld_x3 ~ start(-0.2)*omega_x*cf_y2 ## ld_y3 ~ start(-0.2)*omega_y*cf_x2 ## general_x ~~ general_y ## cf_x1 ~~ cf_y1 ## cf_x1 ~~ general_y ## cf_y1 ~~ general_x And the RICLPM model is, estimateRICLPM(waves = 3, constrain_omega = TRUE, constrain_beta = TRUE) |&gt; cat() ## ## # Random Intercepts ## BX =~ 1*x1 + 1*x2 + 1*x3 ## BY =~ 1*y1 + 1*y2 + 1*y3 ## ## # Within-Person Latent Variables ## p1 =~ 1*x1 ## q1 =~ 1*y1 ## p2 =~ 1*x2 ## q2 =~ 1*y2 ## p3 =~ 1*x3 ## q3 =~ 1*y3 ## ## # Latent Variable Means (Fixed to 0) ## p1 ~ 0*1 ## q1 ~ 0*1 ## ## p2 ~ 0*1 ## q2 ~ 0*1 ## ## p3 ~ 0*1 ## q3 ~ 0*1 ## ## # Autoregressive and Cross-lagged Effects ## p2 ~ beta_y*p1 + omega_xy*q1 ## q2 ~ beta_x*q1 + omega_yx*p1 ## p3 ~ beta_y*p2 + omega_xy*q2 ## q3 ~ beta_x*q2 + omega_yx*p2 ## ## # Residual Variances ## p1 ~~ var_p*p1 ## q1 ~~ var_q*q1 ## p2 ~~ var_p*p2 ## q2 ~~ var_q*q2 ## p3 ~~ var_p*p3 ## q3 ~~ var_q*q3 ## ## # Residual Covariances ## p1 ~~ cov_pq*q1 ## p2 ~~ cov_pq*q2 ## p3 ~~ cov_pq*q3 ## ## # Random Intercept Variances and Covariances ## BX ~~ BX ## BY ~~ BY ## BX ~~ BY ## ## # Fix Observed Variable Residuals to Zero ## x1 ~~ 0*x1 ## y1 ~~ 0*y1 ## x2 ~~ 0*x2 ## y2 ~~ 0*y2 ## x3 ~~ 0*x3 ## y3 ~~ 0*y3 And finally, the Bivariate Latent (Linear) Growth Model, estimateLGM(waves = 3, variable_type = &quot;bivariate&quot; ) |&gt; cat() ## # Latent Variables ## p1 =~ 1*x1 ## q1 =~ 1*y1 ## p2 =~ 1*x2 ## q2 =~ 1*y2 ## p3 =~ 1*x3 ## q3 =~ 1*y3 ## ## # Latent Growth Factors for X ## I_x =~ 1*p1 + 1*p2 + 1*p3 ## S_x =~ 0*p1 + 1*p2 + 2*p3 ## ## # Latent Growth Factors for Y ## I_y =~ 1*q1 + 1*q2 + 1*q3 ## S_y =~ 0*q1 + 1*q2 + 2*q3 ## ## # Growth Factor Means ## I_x ~ mean_I_x*1 ## S_x ~ mean_S_x*1 ## I_y ~ mean_I_y*1 ## S_y ~ mean_S_y*1 ## ## # Growth Factor Variances ## I_x ~~ var_I_x*I_x ## S_x ~~ var_S_x*S_x ## I_y ~~ var_I_y*I_y ## S_y ~~ var_S_y*S_y ## ## # Within-Variable Growth Factor Covariances ## I_x ~~ cov_IS_x*S_x ## I_y ~~ cov_IS_y*S_y ## ## # Between-Variable Growth Factor Covariances ## I_x ~~ cov_I_xy*I_y ## S_x ~~ cov_S_xy*S_y ## I_x ~~ cov_IS_xy*S_y ## I_y ~~ cov_IS_yx*S_x ## ## # Latent Variable Means (Fixed to 0) ## p1 ~ 0*1 ## q1 ~ 0*1 ## p2 ~ 0*1 ## q2 ~ 0*1 ## p3 ~ 0*1 ## q3 ~ 0*1 ## ## # Observed Variable Intercepts (Fixed to 0) ## x1 ~ 0*1 ## y1 ~ 0*1 ## x2 ~ 0*1 ## y2 ~ 0*1 ## x3 ~ 0*1 ## y3 ~ 0*1 ## ## # Latent Variable Variances ## p1 ~~ var_p*p1 ## q1 ~~ var_q*q1 ## p2 ~~ var_p*p2 ## q2 ~~ var_q*q2 ## p3 ~~ var_p*p3 ## q3 ~~ var_q*q3 ## ## # Latent Variable Covariances ## p1 ~~ cov_pq*q1 ## p2 ~~ cov_pq*q2 ## p3 ~~ cov_pq*q3 ## ## # Fix Observed Variable Residuals to Zero ## x1 ~~ 0*x1 ## y1 ~~ 0*y1 ## x2 ~~ 0*x2 ## y2 ~~ 0*y2 ## x3 ~~ 0*x3 ## y3 ~~ 0*y3 2.1 Simulation Each estimation function comes with an associated data simulation function. The functions simulate data according to the model specified. These are later used to build Monte Carlo functions. simCLPM() simulates data from a standard cross-lagged panel model. simRICLPM() simulates data from a random-intercept cross-lagged panel model. simLChange() simulates data from a latent change score model. simLGM() simulates data from a latent growth model. Documentation is included for each of the functions and can be accessed with the help() or ? function. 2.2 A Simple Example As a motivating example, let’s simulate data from a CLPM and estimate two regression models, one with AR and CL parameters in “level form” and another regression model on the change scores, also with AR and CL parameters. This is similar to Allison (1990)’s discussion of difference scores versus level scores.The regression models are just OLS models, one using change scores, the other using lags. Not surprisingly, and consistent with the previous section, they are interchangeable. Figure 2.1 shows the DGP and Figure 2.2 shows simulated data. Figure 2.1: The Cross-Lagged Panel Model with Multiple Indicators Figure 2.2: Data Generating Process: CLPM; Simulated trajectories shown Using these data, let’s estimate a change score regression, alongside a level score regression (estimating the CLPM with least squares). These are not latent variable models, but rather simple OLS regressions on observed data. ## ## Call: ## lm(formula = delta_y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4868 -0.6711 -0.0025 0.6651 4.1120 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.02526 0.01612 1.567 0.117 ## y_lag -0.60491 0.01529 -39.552 &lt;2e-16 *** ## x_lag 0.21590 0.01487 14.516 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9967 on 3997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.2813, Adjusted R-squared: 0.2809 ## F-statistic: 782.2 on 2 and 3997 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4868 -0.6711 -0.0025 0.6651 4.1120 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.02526 0.01612 1.567 0.117 ## y_lag 0.39509 0.01529 25.833 &lt;2e-16 *** ## x_lag 0.21590 0.01487 14.516 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9967 on 3997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.2516, Adjusted R-squared: 0.2512 ## F-statistic: 671.7 on 2 and 3997 DF, p-value: &lt; 2.2e-16 If we simulate data under a latent change model, with no constant change, the CLPM and difference parameters should align (they do). Notice that the \\(beta\\) parameter in the CLPM formulation is exactly the same as the difference score interpretation, as \\(\\hat\\beta_{CLPM} - 1 = \\hat{\\beta_\\Delta}\\), where \\(\\hat\\Delta\\) is the the difference score autoregressive parameter, and \\(\\hat\\beta_{CLPM}\\) is the level form autoregressive parameter. Likewise, we could simulate data under a latent change score specification (with no constant effects), and again the models mirror the DGP parameters and are related. ## ## Call: ## lm(formula = delta_y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2570 -0.7089 0.0016 0.7104 4.4047 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.005187 0.005280 -0.982 0.326 ## y_lag -0.746183 0.004708 -158.481 &lt;2e-16 *** ## x_lag 0.351695 0.004703 74.785 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.056 on 39997 degrees of freedom ## (10000 observations deleted due to missingness) ## Multiple R-squared: 0.388, Adjusted R-squared: 0.388 ## F-statistic: 1.268e+04 on 2 and 39997 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2570 -0.7089 0.0016 0.7104 4.4047 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.005187 0.005280 -0.982 0.326 ## y_lag 0.253817 0.004708 53.908 &lt;2e-16 *** ## x_lag 0.351695 0.004703 74.785 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.056 on 39997 degrees of freedom ## (10000 observations deleted due to missingness) ## Multiple R-squared: 0.2527, Adjusted R-squared: 0.2527 ## F-statistic: 6764 on 2 and 39997 DF, p-value: &lt; 2.2e-16 The estimateCLPM() function may be used to estimate a latent variable CLPM. Notice that if we simulate data under the latent change model (without constant change), the CLPM retrieves the correct estimates, \\(\\hat\\beta_{CLPM} - 1 \\approx -0.7\\) ## lavaan 0.6-20 ended normally after 42 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 33 ## Number of equality constraints 21 ## ## Number of observations 10000 ## ## Model Test User Model: ## ## Test statistic 894.277 ## Degrees of freedom 53 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 =~ ## y1 1.000 ## q1 =~ ## x1 1.000 ## p2 =~ ## y2 1.000 ## q2 =~ ## x2 1.000 ## p3 =~ ## y3 1.000 ## q3 =~ ## x3 1.000 ## p4 =~ ## y4 1.000 ## q4 =~ ## x4 1.000 ## p5 =~ ## y5 1.000 ## q5 =~ ## x5 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## p2 ~ ## p1 (bt_y) 0.254 0.005 53.847 0.000 ## q1 (omg_x) 0.351 0.005 74.512 0.000 ## q2 ~ ## q1 (bt_x) 0.259 0.005 54.757 0.000 ## p1 (omg_y) 0.344 0.005 72.900 0.000 ## p3 ~ ## p2 (bt_y) 0.254 0.005 53.847 0.000 ## q2 (omg_x) 0.351 0.005 74.512 0.000 ## q3 ~ ## q2 (bt_x) 0.259 0.005 54.757 0.000 ## p2 (omg_y) 0.344 0.005 72.900 0.000 ## p4 ~ ## p3 (bt_y) 0.254 0.005 53.847 0.000 ## q3 (omg_x) 0.351 0.005 74.512 0.000 ## q4 ~ ## q3 (bt_x) 0.259 0.005 54.757 0.000 ## p3 (omg_y) 0.344 0.005 72.900 0.000 ## p5 ~ ## p4 (bt_y) 0.254 0.005 53.847 0.000 ## q4 (omg_x) 0.351 0.005 74.512 0.000 ## q5 ~ ## q4 (bt_x) 0.259 0.005 54.757 0.000 ## p4 (omg_y) 0.344 0.005 72.900 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 ~~ ## q1 (cv_1) 0.309 0.015 20.258 0.000 ## .p2 ~~ ## .q2 (cv_x) 0.322 0.006 55.368 0.000 ## .p3 ~~ ## .q3 (cv_x) 0.322 0.006 55.368 0.000 ## .p4 ~~ ## .q4 (cv_x) 0.322 0.006 55.368 0.000 ## .p5 ~~ ## .q5 (cv_x) 0.322 0.006 55.368 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 0.008 0.012 0.636 0.525 ## q1 0.004 0.012 0.336 0.737 ## .p2 0.000 ## .q2 0.000 ## .p3 0.000 ## .q3 0.000 ## .p4 0.000 ## .q4 0.000 ## .p5 0.000 ## .q5 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 (vr_y1) 1.503 0.021 70.711 0.000 ## q1 (vr_x1) 1.487 0.021 70.711 0.000 ## .p2 (vr_y) 1.115 0.008 141.421 0.000 ## .q2 (vr_x) 1.121 0.008 141.421 0.000 ## .p3 (vr_y) 1.115 0.008 141.421 0.000 ## .q3 (vr_x) 1.121 0.008 141.421 0.000 ## .p4 (vr_y) 1.115 0.008 141.421 0.000 ## .q4 (vr_x) 1.121 0.008 141.421 0.000 ## .p5 (vr_y) 1.115 0.008 141.421 0.000 ## .q5 (vr_x) 1.121 0.008 141.421 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 2.3 Summary: Change versus Level Scores Consistent with Allison (1990), there is a similarity between these two approaches – modeling dynamic processes in change and level form. However, the simulated examples thus far ignore an important dynamic, unobserved heterogeneity or unit effects. Hsiao (2022) shows that when autoregressive parameters are specified, and unit effects are ignored, the CLPM specification will yield biased parameter estimates, overestimating the autoregressive parameter and under-estimating the cross-lagged parameter. In the next section, we more fully consider the empirical consequences of unobserved heterogeneity and how latent variable models may be specified to estimated these random effects. 2.4 Unobserved Heterogeneity The CLPM and Change Score models estimated will produce biased parameter estimates, as unit effects are not considered. Both the RI-CLPM and the Latent Change Score model (LCM) address these concerns, by decomposing the variance in \\(y\\) to between (i.e., unit) and within (i.e., variation around the unit’s mean) in the case of the RI-CLPM versus a model that models constant change (i.e., unit change) and proportional change in the case of the LCM. Let’s begin by considering how the CLPM does not correctly retrieve parameter estimates assuming a LCM DGP with constant effects. The DGP is a Latent Change Model with Constant Effects and the Estimator is the CLPM Assume the DGP is a latent change model with constant effects. The data are generated following 2.3. Next, we estimate an ordinary CLPM. The autoregressive parameter (\\(\\hat\\beta \\approx 0.3\\)) is biased and is \\(\\approx 0.7\\). The cross lagged parameters (\\(\\hat \\omega \\approx 0.4\\)) where they should be closer to 0.5. Figure 2.3: DGP: Latent Change with Unit Effects; Estimator: CLPM. ## In this example, the DGP is a latent change model ## with constant effects. The estimator is a CLPM model, ## which doesn&#39;t fully control for unit effects. ## lavaan results are printed below ## lavaan 0.6-20 ended normally after 44 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 33 ## Number of equality constraints 21 ## ## Number of observations 10000 ## ## Model Test User Model: ## ## Test statistic 21439.095 ## Degrees of freedom 53 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 =~ ## y1 1.000 ## q1 =~ ## x1 1.000 ## p2 =~ ## y2 1.000 ## q2 =~ ## x2 1.000 ## p3 =~ ## y3 1.000 ## q3 =~ ## x3 1.000 ## p4 =~ ## y4 1.000 ## q4 =~ ## x4 1.000 ## p5 =~ ## y5 1.000 ## q5 =~ ## x5 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## p2 ~ ## p1 (bt_y) 0.710 0.004 161.170 0.000 ## q1 (omg_x) 0.434 0.004 98.300 0.000 ## q2 ~ ## q1 (bt_x) 0.708 0.004 160.614 0.000 ## p1 (omg_y) 0.435 0.004 98.793 0.000 ## p3 ~ ## p2 (bt_y) 0.710 0.004 161.170 0.000 ## q2 (omg_x) 0.434 0.004 98.300 0.000 ## q3 ~ ## q2 (bt_x) 0.708 0.004 160.614 0.000 ## p2 (omg_y) 0.435 0.004 98.793 0.000 ## p4 ~ ## p3 (bt_y) 0.710 0.004 161.170 0.000 ## q3 (omg_x) 0.434 0.004 98.300 0.000 ## q4 ~ ## q3 (bt_x) 0.708 0.004 160.614 0.000 ## p3 (omg_y) 0.435 0.004 98.793 0.000 ## p5 ~ ## p4 (bt_y) 0.710 0.004 161.170 0.000 ## q4 (omg_x) 0.434 0.004 98.300 0.000 ## q5 ~ ## q4 (bt_x) 0.708 0.004 160.614 0.000 ## p4 (omg_y) 0.435 0.004 98.793 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 ~~ ## q1 (cv_1) 0.295 0.015 19.500 0.000 ## .p2 ~~ ## .q2 (cv_x) 0.470 0.008 62.280 0.000 ## .p3 ~~ ## .q3 (cv_x) 0.470 0.008 62.280 0.000 ## .p4 ~~ ## .q4 (cv_x) 0.470 0.008 62.280 0.000 ## .p5 ~~ ## .q5 (cv_x) 0.470 0.008 62.280 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 -0.010 0.012 -0.819 0.413 ## q1 -0.011 0.012 -0.906 0.365 ## .p2 0.000 ## .q2 0.000 ## .p3 0.000 ## .q3 0.000 ## .p4 0.000 ## .q4 0.000 ## .p5 0.000 ## .q5 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 (vr_y1) 1.483 0.021 70.711 0.000 ## q1 (vr_x1) 1.486 0.021 70.711 0.000 ## .p2 (vr_y) 1.438 0.010 141.421 0.000 ## .q2 (vr_x) 1.433 0.010 141.421 0.000 ## .p3 (vr_y) 1.438 0.010 141.421 0.000 ## .q3 (vr_x) 1.433 0.010 141.421 0.000 ## .p4 (vr_y) 1.438 0.010 141.421 0.000 ## .q4 (vr_x) 1.433 0.010 141.421 0.000 ## .p5 (vr_y) 1.438 0.010 141.421 0.000 ## .q5 (vr_x) 1.433 0.010 141.421 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 What about the RI-CLPM? If we simulate data under the RI-CLPM DGP, does the CLPM retrieve the correct parameters? ## In this example, the DGP is the RICLPM ## The estimator is a CLPM model, ## which doesn&#39;t fully control for unit effects. ## lavaan results are printed below ## lavaan 0.6-20 ended normally after 44 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 33 ## Number of equality constraints 21 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 198.728 ## Degrees of freedom 53 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 =~ ## y1 1.000 ## q1 =~ ## x1 1.000 ## p2 =~ ## y2 1.000 ## q2 =~ ## x2 1.000 ## p3 =~ ## y3 1.000 ## q3 =~ ## x3 1.000 ## p4 =~ ## y4 1.000 ## q4 =~ ## x4 1.000 ## p5 =~ ## y5 1.000 ## q5 =~ ## x5 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## p2 ~ ## p1 (bt_y) 0.435 0.014 30.165 0.000 ## q1 (omg_x) 0.239 0.014 16.651 0.000 ## q2 ~ ## q1 (bt_x) 0.436 0.015 30.053 0.000 ## p1 (omg_y) 0.251 0.015 17.221 0.000 ## p3 ~ ## p2 (bt_y) 0.435 0.014 30.165 0.000 ## q2 (omg_x) 0.239 0.014 16.651 0.000 ## q3 ~ ## q2 (bt_x) 0.436 0.015 30.053 0.000 ## p2 (omg_y) 0.251 0.015 17.221 0.000 ## p4 ~ ## p3 (bt_y) 0.435 0.014 30.165 0.000 ## q3 (omg_x) 0.239 0.014 16.651 0.000 ## q4 ~ ## q3 (bt_x) 0.436 0.015 30.053 0.000 ## p3 (omg_y) 0.251 0.015 17.221 0.000 ## p5 ~ ## p4 (bt_y) 0.435 0.014 30.165 0.000 ## q4 (omg_x) 0.239 0.014 16.651 0.000 ## q5 ~ ## q4 (bt_x) 0.436 0.015 30.053 0.000 ## p4 (omg_y) 0.251 0.015 17.221 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 ~~ ## q1 (cv_1) 0.608 0.050 12.102 0.000 ## .p2 ~~ ## .q2 (cv_x) 0.185 0.017 10.625 0.000 ## .p3 ~~ ## .q3 (cv_x) 0.185 0.017 10.625 0.000 ## .p4 ~~ ## .q4 (cv_x) 0.185 0.017 10.625 0.000 ## .p5 ~~ ## .q5 (cv_x) 0.185 0.017 10.625 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 -0.010 0.039 -0.253 0.800 ## q1 0.014 0.038 0.374 0.708 ## .p2 0.000 ## .q2 0.000 ## .p3 0.000 ## .q3 0.000 ## .p4 0.000 ## .q4 0.000 ## .p5 0.000 ## .q5 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 (vr_y1) 1.505 0.067 22.361 0.000 ## q1 (vr_x1) 1.433 0.064 22.361 0.000 ## .p2 (vr_y) 1.078 0.024 44.721 0.000 ## .q2 (vr_x) 1.099 0.025 44.721 0.000 ## .p3 (vr_y) 1.078 0.024 44.721 0.000 ## .q3 (vr_x) 1.099 0.025 44.721 0.000 ## .p4 (vr_y) 1.078 0.024 44.721 0.000 ## .q4 (vr_x) 1.099 0.025 44.721 0.000 ## .p5 (vr_y) 1.078 0.024 44.721 0.000 ## .q5 (vr_x) 1.099 0.025 44.721 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 Notice, the same problem exists. The model does a poor job retrieving the correct parameter estimates. Why does the CLPM get it wrong? It only does as the proportion of the variance in \\(y\\) (or \\(x\\)) increases due to unit effect – unobserved heterogeneity. The CLPM assumes that all variance is wave-specific, and thus misattributes stable variance in \\(y\\) to the autoregression parameters. This can be seen in examining the simulated trajectories of data generated varying the variance parameters for the random intercepts. First, let’s consider a situation in which the between variance is 4 times as large as the within variance. Figure 2.4: Individual trajectories under RICLPM This can be compared to the same DGP but where the within variance is 4 times as large as the between variance. library(dplyr) library(ggplot2) library(crossLagR) library(lavaan) wide_data = simRICLPM( waves = 5, beta_x = 0.3, beta_y = 0.8, sample.nobs = 1000, var_BX = 0.25, var_BY = 0.25, var_p = 1, var_q = 1, estimate_means = TRUE)$data ## Warning: lavaan-&gt;lav_start_check_cov(): ## starting values imply a correlation larger than 1; variables involved are: ## BX BY ## ✅ Successfully simulated RICLPM data with 5 waves and 1000 observations. data = wide_data |&gt; reshape_long_sim_cr() |&gt; group_by(id) |&gt; mutate( y_lag = lag(y), x_lag = lag(x), delta_y = y - lag(y) ) |&gt; ungroup() -&gt; data data |&gt; ggplot(aes(x = wave, y = y, group = id)) + geom_line(alpha = 0.1, color = &quot;black&quot;, position = position_jitter(width = 0.1, height = 0.2)) + stat_summary(aes(group = 1), fun = mean, geom = &quot;line&quot;, color = &quot;lightblue&quot;, size = 1, alpha = 0.4) + stat_summary(aes(group = 1), fun = mean, geom = &quot;point&quot;, color = &quot;lightblue&quot;, size = 1) + labs( title = &quot;Individual Trajectories: Simulated Data&quot;, subtitle = &quot;Within Variance = 4x as large as Between&quot;, x = &quot;Survey Wave&quot;, y = &quot;Response&quot; ) + theme_minimal() + scale_y_continuous(limits = c(-5, 6) ) + theme(strip.text = element_text(color = &quot;black&quot;)) -&gt; plot print(plot) Figure 2.5: Individual trajectories under RICLPM Figure 2.4 shows real differences between units – they’re spread out; whereas Figure 2.5 shows much less difference between units – they’re more tightly compact. The fundamental relationship between the points over time does not change, all that changes is the spread of the trajectories. 2.5 Model Validation It may be useful to show these models can be estimated using techniques more familiar to a political science audience. Below, I estimate both the latent change model with constant effects and the RI-CLPM using brms. 2.5.1 Hierarchical brms Estimation This is the differenced variable regression with random intercepts. summary(model) ## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be ## careful when analysing the results! We recommend running more iterations and/or ## setting stronger priors. ## Family: MV(gaussian, gaussian) ## Links: mu = identity ## mu = identity ## Formula: delta_y ~ -1 + y_lag + x_lag + (1 | id) ## delta_x ~ -1 + y_lag + x_lag + (1 | id) ## Data: data (Number of observations: 29000) ## Draws: 3 chains, each with iter = 1000; warmup = 500; thin = 1; ## total post-warmup draws = 1500 ## ## Multilevel Hyperparameters: ## ~id (Number of levels: 1000) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## sd(deltay_Intercept) 1.69 0.04 1.62 1.78 1.06 59 ## sd(deltax_Intercept) 1.71 0.05 1.63 1.81 1.10 24 ## Tail_ESS ## sd(deltay_Intercept) 202 ## sd(deltax_Intercept) 54 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## deltay_y_lag -0.83 0.01 -0.84 -0.82 1.00 2942 1135 ## deltay_x_lag 0.20 0.01 0.18 0.21 1.00 934 1224 ## deltax_y_lag 0.19 0.01 0.18 0.20 1.00 779 888 ## deltax_x_lag -0.83 0.01 -0.84 -0.82 1.00 2726 947 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_deltay 1.06 0.00 1.05 1.07 1.00 2495 1213 ## sigma_deltax 1.06 0.00 1.05 1.07 1.00 2309 931 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## rescor(deltay,deltax) 0.11 0.01 0.10 0.12 1.01 3792 ## Tail_ESS ## rescor(deltay,deltax) 726 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 2.6 Short t bias. It is well established that estimating a dynamic fixed effects model with short t – relatively few waves – can lead to biased estimates. This is shown in Hsiao (2022). To see this, here is a dynamic panel data simulation where the DGP is an RI-CLPM with known parameters. 2.7 The Fixed Effect (Dynamic Panel) Estimator This example simulates data from a four wave panel. Notice that the parameter estimates are far from their true values. library(crossLagR) library(dplyr) library(purrr) library(tidyr) # Long data baseRICLPM_sim(n = 1000, ar_x = 0.5, ar_y = 0.5, cl_xy = 0.3, cl_yx = 0.2, waves = 4) |&gt; select(-contains(&quot;trait&quot;)) |&gt; # reshape to long reshape_long_sim_cr() -&gt; dat lm(within.y ~ xlagw + ylagw, data = dat) |&gt; summary() ## ## Call: ## lm(formula = within.y ~ xlagw + ylagw, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.84701 -0.36315 0.00182 0.36254 2.49156 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.002764 0.009604 -0.288 0.774 ## xlagw 0.174539 0.018460 9.455 &lt;2e-16 *** ## ylagw -0.163348 0.018381 -8.887 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.526 on 2997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.04559, Adjusted R-squared: 0.04495 ## F-statistic: 71.57 on 2 and 2997 DF, p-value: &lt; 2.2e-16 With more waves \\(t = 30\\), the model recovers the true parameters. #| echo: true #| message: false #| warning: false # Long data library(crossLagR) library(dplyr) library(purrr) library(tidyr) baseRICLPM_sim(n = 1000, ar_x = 0.5, ar_y = 0.5, cl_xy = 0.3, cl_yx = 0.2, waves = 30) |&gt; select(-contains(&quot;trait&quot;)) |&gt; # reshape to long reshape_long_sim_cr() -&gt; dat lm(within.y ~ xlagw + ylagw, data = dat) |&gt; summary() ## ## Call: ## lm(formula = within.y ~ xlagw + ylagw, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.81152 -0.47215 -0.00156 0.47284 2.58771 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.001252 0.004083 -0.307 0.759 ## xlagw 0.299270 0.005733 52.200 &lt;2e-16 *** ## ylagw 0.441164 0.005381 81.979 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6953 on 28997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.3973, Adjusted R-squared: 0.3973 ## F-statistic: 9559 on 2 and 28997 DF, p-value: &lt; 2.2e-16 2.8 The RICLPM The RICLPM doesn’t exhibit the same problem, with minimal bias (that decreases) with more waves. The code below runs a simple Monte Carlo analysis varying the number of panel waves, and the following fixed parameters (\\(n = 2000; \\beta_x = 0.4; \\beta_y = 0.7; \\omega_{x \\rightarrow y} = 0.2; \\omega_{y \\rightarrow x} = 0.3\\)). library(crossLagR) library(dplyr) library(purrr) library(ggplot2) library(tidyr) true_params &lt;- list(beta_x = 0.4, beta_y = 0.7, omega_xy = 0.2, omega_yx = 0.3) extract_params &lt;- function(fit) { pe &lt;- lavaan::parameterEstimates(fit) factor_scores &lt;- lavaan::lavPredict(fit, method = &quot;EBM&quot;) factor_scores_df &lt;- as_tibble(factor_scores) %&gt;% select(starts_with(&quot;general_y&quot;) | starts_with(&quot;general_x&quot;)) tibble( beta_x = pe$est[pe$label == &quot;beta_x&quot;][1], beta_y = pe$est[pe$label == &quot;beta_y&quot;][1], omega_xy = pe$est[pe$label == &quot;omega_xy&quot;][1], omega_yx = pe$est[pe$label == &quot;omega_yx&quot;][1], ) } results &lt;- expand_grid( waves = c(3, 5, 7, 10, 30), rep = 1:50 ) |&gt; mutate( data = map(waves, ~{ baseRICLPM_sim( n = 2000, innov_var = 0.45, ar_x = 0.4, ar_y = 0.7, cl_xy = 0.3, cl_yx = 0.2, waves = .x ) |&gt; select(-contains(&quot;trait&quot;)) }), # This iterates over the data, x and waves y fit = map2(data, waves, ~{ lavaan::lavaan(estimateRICLPM(waves = ..2, label_autoregressive = c(&quot;beta_x&quot;, &quot;beta_y&quot;)), data = ..1, warn = FALSE, verbose = FALSE) }), params = map(fit, extract_params) ) |&gt; unnest(params) library(ggridges) library(dplyr) library(tidyr) library(ggplot2) library(ggridges) # Prepare data with simpler labels for parsing results_long &lt;- results |&gt; select(waves, rep, beta_x, beta_y, omega_xy, omega_yx) |&gt; pivot_longer(cols = c(beta_x, beta_y, omega_xy, omega_yx), names_to = &quot;parameter&quot;, values_to = &quot;estimate&quot;) |&gt; mutate( parameter_label = case_when( parameter == &quot;beta_x&quot; ~ &quot;Autoregressive[x]&quot;, parameter == &quot;beta_y&quot; ~ &quot;Autoregressive[y]&quot;, parameter == &quot;omega_xy&quot; ~ &quot;CrossLag~X%-&gt;%Y~(omega[xy])&quot;, parameter == &quot;omega_yx&quot; ~ &quot;CrossLag~Y%-&gt;%X~(omega[yx])&quot; ), true_value = case_when( parameter == &quot;beta_x&quot; ~ 0.4, parameter == &quot;beta_y&quot; ~ 0.7, parameter == &quot;omega_xy&quot; ~ 0.2, parameter == &quot;omega_yx&quot; ~ 0.3 ), waves_factor = factor(waves, levels = c(30, 10, 7, 5, 3)) ) # Calculate medians for each wave/parameter combination medians &lt;- results_long |&gt; group_by(waves_factor, parameter_label, true_value) |&gt; summarise(median_est = median(estimate), .groups = &quot;drop&quot;) # Create plot with median lines ggplot(results_long, aes(x = estimate, y = waves_factor)) + geom_density_ridges( aes(fill = waves_factor), scale = 2, rel_min_height = 0.01, alpha = 0.7 ) + geom_vline(aes(xintercept = true_value), linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 0.8, alpha = 0.5) + geom_segment(data = medians, aes(x = median_est, xend = median_est, y = as.numeric(waves_factor) - 0.3, yend = as.numeric(waves_factor) + 0.3), color = &quot;green&quot;, size = 1) + facet_wrap(~parameter_label, scales = &quot;free_x&quot;, labeller = label_parsed, nrow = 2) + scale_fill_viridis_d(option = &quot;viridis&quot;, begin = 0.2, end = 0.9) + labs( title = &quot;Monte Carlo Sample Distribution Varying Waves, RI-CLPM&quot;, subtitle = &quot;Green line = median estimate | Dashed line = true value&quot;, x = &quot;Parameter Estimate&quot;, y = &quot;Number of Waves&quot; ) + theme_minimal(base_size = 12) + theme( legend.position = &quot;none&quot;, strip.text = element_text(size = 11, face = &quot;bold&quot;), panel.grid.minor = element_blank() ) ## Picking joint bandwidth of 0.00655 ## Picking joint bandwidth of 0.00496 ## Picking joint bandwidth of 0.00486 ## Picking joint bandwidth of 0.00515 Here is a summary table, library(gt) bias_summary &lt;- results |&gt; group_by(waves) |&gt; summarise( across(c(beta_x, beta_y, omega_xy, omega_yx), list( mean = mean, bias = ~mean(.) - true_params[[cur_column()]], relative_bias = ~ (mean(.) - true_params[[cur_column()]]) / true_params[[cur_column()]]*100, se = ~sd(.) ), .names = &quot;{.col}_{.fn}&quot;) ) |&gt; select(waves, contains(&quot;bias&quot;)) |&gt; mutate(across(where(is.numeric), ~round(., 3))) bias_summary |&gt; gt() |&gt; tab_header( title = md(&quot;**Monte Carlo Bias Analysis**&quot;), subtitle = &quot;RICLPM Estimation: Varying the Number of Waves&quot; ) |&gt; cols_label( waves = &quot;Waves&quot;, beta_x_bias = &quot;Bias, AR(X)&quot;, beta_y_bias = &quot;Bias, AR(Y)&quot;, beta_x_relative_bias = &quot;Relative Bias AR(X) %&quot;, beta_y_relative_bias = &quot;Relative Bias AR(Y) %&quot;, omega_xy_bias = &quot;Bias, CL(X→Y)&quot;, omega_yx_bias = &quot;Bias, CL(Y→X)&quot;, omega_xy_relative_bias = &quot;Relative Bias, CL(X→Y) %&quot;, omega_yx_relative_bias = &quot;Relative Bias, CL(Y→X) %&quot; )|&gt; tab_source_note( source_note = md(&quot;*Note*: The results reported here are from 100 simulations, varying the number of waves.&quot;) ) |&gt; tab_footnote( footnote = &quot;Bias = Mean(estimate) - True value, Relative Bias = (Bias / True value) * 100&quot;) #magnxtnixu table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #magnxtnixu thead, #magnxtnixu tbody, #magnxtnixu tfoot, #magnxtnixu tr, #magnxtnixu td, #magnxtnixu th { border-style: none; } #magnxtnixu p { margin: 0; padding: 0; } #magnxtnixu .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #magnxtnixu .gt_caption { padding-top: 4px; padding-bottom: 4px; } #magnxtnixu .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #magnxtnixu .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #magnxtnixu .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #magnxtnixu .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #magnxtnixu .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #magnxtnixu .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #magnxtnixu .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #magnxtnixu .gt_column_spanner_outer:first-child { padding-left: 0; } #magnxtnixu .gt_column_spanner_outer:last-child { padding-right: 0; } #magnxtnixu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #magnxtnixu .gt_spanner_row { border-bottom-style: hidden; } #magnxtnixu .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #magnxtnixu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #magnxtnixu .gt_from_md > :first-child { margin-top: 0; } #magnxtnixu .gt_from_md > :last-child { margin-bottom: 0; } #magnxtnixu .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #magnxtnixu .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #magnxtnixu .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #magnxtnixu .gt_row_group_first td { border-top-width: 2px; } #magnxtnixu .gt_row_group_first th { border-top-width: 2px; } #magnxtnixu .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #magnxtnixu .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #magnxtnixu .gt_first_summary_row.thick { border-top-width: 2px; } #magnxtnixu .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #magnxtnixu .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #magnxtnixu .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #magnxtnixu .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #magnxtnixu .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #magnxtnixu .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #magnxtnixu .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #magnxtnixu .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #magnxtnixu .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #magnxtnixu .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #magnxtnixu .gt_left { text-align: left; } #magnxtnixu .gt_center { text-align: center; } #magnxtnixu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #magnxtnixu .gt_font_normal { font-weight: normal; } #magnxtnixu .gt_font_bold { font-weight: bold; } #magnxtnixu .gt_font_italic { font-style: italic; } #magnxtnixu .gt_super { font-size: 65%; } #magnxtnixu .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #magnxtnixu .gt_asterisk { font-size: 100%; vertical-align: 0; } #magnxtnixu .gt_indent_1 { text-indent: 5px; } #magnxtnixu .gt_indent_2 { text-indent: 10px; } #magnxtnixu .gt_indent_3 { text-indent: 15px; } #magnxtnixu .gt_indent_4 { text-indent: 20px; } #magnxtnixu .gt_indent_5 { text-indent: 25px; } #magnxtnixu .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #magnxtnixu div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Monte Carlo Bias Analysis RICLPM Estimation: Varying the Number of Waves Waves Bias, AR(X) Relative Bias AR(X) % Bias, AR(Y) Relative Bias AR(Y) % Bias, CL(X→Y) Relative Bias, CL(X→Y) % Bias, CL(Y→X) Relative Bias, CL(Y→X) % 3 0.001 0.152 0.002 0.270 0.002 1.149 -0.005 -1.562 5 -0.002 -0.581 0.001 0.190 0.001 0.471 0.001 0.186 7 0.001 0.341 -0.001 -0.176 -0.001 -0.278 0.001 0.248 10 0.003 0.666 -0.001 -0.141 -0.002 -0.850 -0.001 -0.314 30 0.001 0.145 0.000 -0.006 0.000 -0.101 0.002 0.531 Bias = Mean(estimate) - True value, Relative Bias = (Bias / True value) * 100 Note: The results reported here are from 100 simulations, varying the number of waves. 2.9 Change Below I attempt to do the same analysis but with the latent change score model with constant effects. In these simulations, I vary the degree of “between-unit” and “within-wave” variation. I didn’t have much luck here….strange results. library(dplyr) library(purrr) library(tidyr) baseRICLPM_sim(n = 1000, ar_x = 0.4, ar_y = 0.7, cl_xy = 0.3, cl_yx = 0.2, waves = 10) |&gt; select(-contains(&quot;trait&quot;)) -&gt; dat fit = lavaan::lavaan(estimateLChange(waves = 10, variable_type = &quot;bivariate&quot;), data = dat, warn = FALSE, verbose = FALSE) true_params &lt;- list(beta_x = -0.6 , beta_y = -0.3, omega_x = 0.2, omega_y = 0.3) extract_params &lt;- function(fit) { pe &lt;- lavaan::parameterEstimates(fit) tibble( beta_x = pe$est[pe$label == &quot;beta_x&quot;][1], beta_y = pe$est[pe$label == &quot;beta_y&quot;][1], omega_x = pe$est[pe$label == &quot;omega_x&quot;][1], omega_y = pe$est[pe$label == &quot;omega_y&quot;][1] ) } results &lt;- expand_grid( waves = c(3, 5, 10, 30), innov_var = seq(0.25, 1.25, by = 1), rep = 1:50 ) |&gt; mutate( data = pmap(list(waves = waves, innov_var = innov_var), ~{ baseRICLPM_sim( n = 2000, innov_var = ..2, # second arg pmap ar_x = 0.4, ar_y = 0.7, cl_xy = 0.3, cl_yx = 0.2, waves = ..1 # first arg in pmap ) }), # This iterates over the data, x and waves y fit = map2(data, waves, ~{ lavaan::lavaan(estimateLChange(waves = ..2, variable_type = &quot;bivariate&quot;, estimate_constant_change = FALSE), data = ..1, warn = FALSE, verbose = FALSE) }), params = map(fit, extract_params) ) |&gt; unnest(params) results |&gt; mutate( beta_y = beta_y , beta_x = beta_x ) |&gt; group_by(waves, innov_var) |&gt; summarise( across(c(beta_x, beta_y, omega_y, omega_x), list( mean = mean, bias = ~mean(.) - true_params[[cur_column()]], relative_bias = ~ (mean(.) - true_params[[cur_column()]]) / true_params[[cur_column()]]*100, se = ~sd(.) ), .names = &quot;{.col}_{.fn}&quot;)) -&gt; bias_summary library(gt) bias_summary |&gt; select(waves, contains(c(&quot;mean&quot;, &quot;waves&quot;, &quot;innov_var&quot;))) |&gt; mutate(across(where(is.numeric), ~round(., 3))) ## # A tibble: 8 × 6 ## # Groups: waves [4] ## waves beta_x_mean beta_y_mean omega_y_mean omega_x_mean innov_var ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 -0.035 -0.044 0.057 0.013 0.25 ## 2 3 -0.183 -0.169 0.208 0.068 1.25 ## 3 5 -0.024 -0.049 0.045 -0.001 0.25 ## 4 5 -0.141 -0.184 0.197 0.034 1.25 ## 5 10 -0.009 -0.058 0.033 -0.016 0.25 ## 6 10 -0.096 -0.194 0.186 0.001 1.25 ## 7 30 0.001 -0.059 0.018 -0.025 0.25 ## 8 30 -0.049 -0.181 0.151 -0.026 1.25 2.10 Summary The change (LCM) and level score (CLPM; RICLPM) analysis are equivalent in the absence of unit effects. This suggests a close relationship between these models. However, when unit effects are present, the models diverge. When the DGP includes unit effects, the same interchangeable relationships disappear. References Allison, Paul D. 1990. “Change Scores as Dependent Variables in Regression Analysis.” Sociological Methodology, 93–114. Hamaker, Ellen L., Rebecca M. Kuiper, and Raoul P. P. P. Grasman. 2015. “A Critique of the Cross-Lagged Panel Model.” Psychological Methods 20 (1): 102–16. https://doi.org/10.1037/a0038889. Hsiao, Cheng. 2022. Analysis of Panel Data. 64. Cambridge university press. "]]

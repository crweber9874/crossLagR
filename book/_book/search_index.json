[["index.html", "Cross Lagged Regression Preface", " Cross Lagged Regression Christopher Weber 2025-11-28 Preface The Cross-Lagged Panel Model (CLPM) is widely accepted as a standard method for longitudinal causal analysis in political science. Its application, however, is often susceptible to unaccounted time-invariant confounders and measurement error, leading to spurious estimates of autoregressive and cross-lagged relationships, as well as bias in statistical inferences and substantive conclusions. In these documentation files, I demonstrate the similarities and differences between different panel analysis models, showing in particular, that the standard CLPM is strongly related to a more flexible and robust alternatives, the change score regression model and the random-intercept cross-lagged regression model. Both the Latent Change Model (LCM) and the Random Intercept Cross-Lagged Panel Model (RI-CLPM) extend the standard CLPM by explicitly modeling “unit level” effects. In simulations and empirical examples, we show the myriad paths by which the CLPM produces biased parameter estimates due to unmodeled, stable-unit effects. Both approaches allow for “constant” differences between units over time: In the RI-CLPM, this is represented by a factor corresponding to unit-mean variation in the observed variables, a “random-intercept” while in the LCM, this is captured through a factor corresponding to unit-level change over time, a “random slope.” This documentation elaborates upon: The theoretical background for the CLPM, RI-CLPM, and LCM, highlighting their similarities and differences. An introduction to the crossLagR package, which includes various utilities to estimate and simulate effects from these models. A Monte Carlo simulation procedure that examines the degree of bias in parameter estimates from the CLPM, RI-CLPM, and LCM under different data-generating processes. "],["panel-data-models.html", "Chapter 1 Panel Data Models 1.1 The Cross-Lagged Panel Model 1.2 The Change Score Regression 1.3 Example: CLPM and Change Score Regression 1.4 Types of Change 1.5 Unit Effects 1.6 The Random Intercept Cross Lagged Regression 1.7 The Latent Growth Model 1.8 Measurement Error", " Chapter 1 Panel Data Models Social scientists often turn to panel data to diagnose causal effects when experiments are impractical (Chiu et al. 2025). Whereas dozens of options exist for analyzing longer time series, researchers wishing to analyze cross-sectionally dominant panel structures – in which there are more units than time points – face a more limited set of choices. National surveys like found in the American National Election Studies (ANES) may consist of just a handful of waves, with many respondents interviewed only once or twice. We consider of the popular methods for analyzing panel data in political science. We start by focusing on a common approach to estimate dynamic processes in panel data (particularly public opinion panel data that are “cross section” dominant, with far more respondents than panel waves: the cross-lagged panel model (CLPM). The CLPM – introduced by Stanley and Campbell (1963) and extended on by Kenny (1975) – has been widely used to study reciprocal relationships between variables over time. These methods first appeared in the political science literature in the late 1970s [e.g, ;Abramowitz (1978); Campbell and Meier (1979); Iyengar (1978); Jennings and Markus (1977); Markus (1979b); Markus and Converse (1979),Meier and Campbell (1979)]. The availability of two large panel surveys with rich political content – Jennings and Niemi’s Jennings and Niemi (1974) socialization study and the ANES 1972-1974-1976 panel – likely promoted the CLPM’s adoption, as most of the cross-lagged analyses published in this era analyze these two datasets. Another factor leading to the widespread adoption of the CLPM was the availability of Jöreskog’s LISREL software, which allowed researchers to quickly and easily solve systems of equations using maximum likelihood (Markus 1979a; McCullough 1978; Shingles 1976). Much of the CLPM’s appeal lies in its intuitive structure, flexibility, and ease of implementation. In the half century after its introduction, the CLPM has also been refined and extended, due to important limitations in its original formulation. For instance, the model can account for unobserved heterogeneity across individuals – stable characteristics of individuals that confound the “dynamic” estimates in the CLPM (Usami, Hayes, and McArdle 2015; Usami 2021; Klopack and Wickrama 2020; Kim and Steiner 2021; Lüdtke and Robitzsch 2022; McArdle 2009). Because the CLPM does not account for stable unit effects, the cross-lagged estimates will tend to be biased (upwards), as they conflate within-person dynamics with between-person differences. Though not directly referenced as a critique of the CLPM, Hsiao (2022) shows that including a lagged dependent variable will produce a biased estimate of the lagged effect, because the variable is correlated with the error term. The correlation arises from the failure to account for “between unit” heterogeneity (pp. 64 -67). As a consequence, CLPM estimates may be biased, especially when the variables of interest exhibit high stability over time (Hamaker, Kuiper, and Grasman 2015; Usami, Hayes, and McArdle 2015; Lüdtke and Robitzsch 2022). This problem is elaborated on in@hsiao2022. Dynamic models – models that include lagged dependent variables – often result in correlations between the observed variables and the error terms, leading to biased estimates. Hsiao (2022) proposes and instrumental variable approach using lagged values of the dependent variable as instruments. When stable unit effects are ignored, the lagged dependent variable becomes correlated with the error term, violating the exogeneity assumption. This correlation leads to biased estimates of the lagged effect. Another popular extension to the CLPM is the random intercept cross-lagged panel model (RI-CLPM) proposed in (hamaker2015?), which separates within-person dynamics from between-person “trait” effects (Usami, Hayes, and McArdle 2015; Lüdtke and Robitzsch 2022). Another approach is the Latent Change Score Model (LSCM), which focuses more directly on estimating different change processes; the LCSM also accounts for stable unit effects, by accounting for unit averaged change (Usami, Hayes, and McArdle 2015; McArdle 2009; Kim and Steiner 2021). Another alternative is the latent growth model (LGM), which models individual trajectories over time (Usami, Hayes, and McArdle 2015; McArdle and Nesselroade 2003). Ultimately,preference for one model over another is strongly tied to data access and availability. While the CLPM and a restrictive version of the LCSM can be estimated with as few as two waves of data, the RI-CLPM, LGM, and LCSM require more panel waves to properly identify the models. Data with three or more waves of data afford more leverage to estimate dynamic relationships. In this project, we elaborate on these models, beginning with the CLPM and change score approach to modeling dynamic relationships. 1.1 The Cross-Lagged Panel Model The cross-lagged regression model follows an intuitive structure. The current realizations \\(x\\) and \\(y\\) are a function of autoregressive and cross lagged effects. The autoregression terms capture time dependence, while the cross-lagged terms capture how another variable leads to change in the outcome variable. The CLPM can may be estimated as series of simultaneous equations with lagged realizations of the \\(x_i\\) and \\(y_i\\). \\[ \\begin{matrix} y_{it} &amp; = \\alpha_{y} + \\beta_{1}y_{it-1} + \\omega_x x_{it-1} + e_{y,it}\\\\ x_{it} &amp; = \\alpha_{x} + \\beta_{1}x_{it-1} + \\omega_x y_{it-1}+ e_{x,it}\\\\ \\end{matrix} \\] \\(\\alpha\\) correspond to intercepts, which may vary across units (e.g., survey respondents) as well as panel wave. It is common to write the CLPM with mean centered variables, in which case the intercept is 0. The \\(\\beta\\) parameters are autoregressive effects. And the \\(\\omega\\) parameters are cross-lagged effects. Figure 1.1: The Cross-Lagged Panel Model It’s useful to write the CLPM in terms of latent variables – “p” and “q” – for the simple reason that this formulation can easily be extended to models with measurement error, varying factor structures, and multiple indicators per construct. In a “single indicator” latent variable model, \\(y = p\\) and \\(x = q\\). With multiple indicators, the CLPM can account for measurement error. Each indicator is defined by the latent variable, and an error term (the dashed loops). Figure 1.2: The Cross-Lagged Panel Model with Multiple Indicators In addition, insofar as the constructs measured at each time point are modeled as latent variables, the CLPM can be extended to account for correlated errors, factor invariance, and multiple group comparisons. Below we write the CLPM and alternatives in terms of single indicator variables, though importantly, these models can all easily account for multiple indicators. 1.2 The Change Score Regression The CLPM is related to the change score regression model (Allison 1990). The change score regression directly constructs change between time points (e.g, \\(\\Delta y_{i,t} = y_{it} - y{i,t-1}\\)). \\[ \\Delta y_{it} = y_{it} - y_{it-1}\\\\ \\Delta x_{it} = x_{it} - x_{it-1} \\] And \\[ \\begin{matrix} \\Delta y_{it} &amp; = \\alpha_{y} + \\beta_{y}y_{it-1} + \\theta_x x_{it-1} + e_{y,it}\\\\ \\Delta x_{it} &amp; = \\alpha_{x} + \\beta_{x}x_{it-1} + \\theta_x y_{it-1}+ e_{x,it}\\\\ \\end{matrix} \\] The \\(\\beta\\) parameter here corresponds to a “proportional” effect. Does the prior value \\(y_{t-1}\\) influence the amount of change in \\(y\\) from time \\(t-1\\) to time \\(t\\)? \\[ \\begin{matrix} \\text{The CLPM: } &amp;y_{it} = \\beta_{y}y_{t-1} + \\theta_{1y} x_{t-1} + e_{y,it}\\\\ \\text{Subtract } y_{it-1}: &amp;y_{it} - y_{it-1} = \\beta_{y}y_{t-1} + \\theta_{1y} x_{t-1} + e_{y,it} - y_{it-1}\\\\ &amp; \\Delta y_{it} = (\\beta_{y} - 1) y_{t-1} + \\theta_{y} x_{t-1} + e_{y,it}\\\\ &amp; \\Delta y_{it} = \\tilde{\\beta}_y y_{t-1} + \\theta_{y} x_{t-1} + e_{y,it}\\\\ \\text{Where: } &amp;\\tilde{\\beta}_y = \\beta_{x} - 1 \\end{matrix} \\] The change regression and CLPM are interchangeable, and the lagged and cross-lagged effects simply become proportional and cross-proportional effects on change scores. . 1.3 Example: CLPM and Change Score Regression # Simulate a five wave panel, with two variables, x y: simCLPM(waves = 5)$data |&gt; # In crossLagR there is a helper function to transform wide to long reshape_long_sim_cr() |&gt; # Id identifies the subject, so use this to construct lags group_by(id) |&gt; mutate( y_lag = lag(y), x_lag = lag(x), # Change Score delta_y = y - lag(y) ) |&gt; ungroup() -&gt; data head(data) ## # A tibble: 6 × 13 ## id wave x y xlag xlagw ylag ylagw within.x within.y y_lag ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.222 0.631 NA NA NA NA -0.467 -0.211 NA ## 2 1 2 1.83 2.39 0.222 -0.467 0.631 -0.211 1.14 1.54 0.631 ## 3 1 3 0.795 -0.140 1.83 1.14 2.39 1.54 0.106 -0.982 2.39 ## 4 1 4 0.467 1.00 0.795 0.106 -0.140 -0.982 -0.221 0.160 -0.140 ## 5 1 5 0.127 0.331 0.467 -0.221 1.00 0.160 -0.561 -0.511 1.00 ## 6 2 1 -0.267 0.308 NA NA NA NA 0.675 0.503 NA ## # ℹ 2 more variables: x_lag &lt;dbl&gt;, delta_y &lt;dbl&gt; ## ## Call: ## lm(formula = delta_y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4144 -0.6652 0.0092 0.6667 3.3917 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.008332 0.016023 0.52 0.603 ## y_lag -0.617717 0.015808 -39.08 &lt;2e-16 *** ## x_lag 0.194509 0.015763 12.34 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.013 on 3997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.2783, Adjusted R-squared: 0.2779 ## F-statistic: 770.5 on 2 and 3997 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4144 -0.6652 0.0092 0.6667 3.3917 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.008332 0.016023 0.52 0.603 ## y_lag 0.382283 0.015808 24.18 &lt;2e-16 *** ## x_lag 0.194509 0.015763 12.34 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.013 on 3997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.2262, Adjusted R-squared: 0.2258 ## F-statistic: 584.2 on 2 and 3997 DF, p-value: &lt; 2.2e-16 The cross-lagged parameters are equal, the AR parameters by \\(1-\\beta\\). In the latent change score model (an extension of the change score regression), one can explicitly model change as a latent variable, and decompose change into constituent processes, “proportional change,” “constant change,” and “dual change” (Grimm et al. 2012). 1.4 Types of Change The univariate change score regression model allows us to decompose change into three components: constant, proportional, and dual change processes (Grimm et al. 2012). \\[ \\begin{matrix} \\Delta y_{it} &amp; = \\alpha_{y} s_i + \\beta_{1}y_{it-1} + e_{y,it}\\\\ \\Delta x_{it} &amp; = \\alpha_{x} s_i + \\beta_{1}x_{it-1} + e_{x,it}\\\\ \\end{matrix} \\] The \\(\\alpha\\) parameters are the effects of a stable trait level component, \\(s_i\\), on the change scores. The \\(\\alpha\\) parameter captures the effect of \\(s_i\\) on the change score at a particular time point. The \\(\\beta\\) parameters are the proportional effect of \\(y\\) and \\(x\\) at time \\(t-1\\) on the change score at time \\(t\\). The trajectory\\(y\\) and \\(x\\) from time \\(t=1\\) to time \\(t=k\\). \\[ y_{i,k} = y_{i,t=1} + \\sum_{t=2}^{k} \\Delta y_{ik} \\\\ x_{i,k} = x_{i,t=1} + \\sum_{t=2}^{k} \\Delta x_{ik} \\\\ \\] Cross-lagged effects can also be included in the change score regression model, allowing us to examine how change in one variable is influenced by the level of another variable at the prior time point. Cross-lagged coefficients can be interpreted as cross-proportional effects, indicating how much that variable changes the trajectory of change in the dependent variable. \\[ \\begin{matrix} \\Delta y_{it} &amp; = \\alpha_{y} s_i + \\beta_{1}y_{it-1} + \\theta_x x_{it-1} + e_{y,it}\\\\ \\Delta x_{it} &amp; = \\alpha_{x} s_i + \\beta_{1}x_{it-1} + \\theta_x y_{it-1}+ e_{x,it}\\\\ \\end{matrix} \\] 1.5 Unit Effects Some researchers have proposed including unit-level fixed effects in the CLPM to account for stable between-person differences. Following Hsiao (2021), the “fixed effects” model either includes dummy variables for each unit, or equivalently, subtracts the unit average from the dependent variable. Assume the true data generating process is a CLPM with dummy indicator in the error equation, \\(s_i\\): \\[ \\begin{matrix} y_{it} &amp; = \\alpha_{y} + \\beta_{1}y_{it-1} + \\omega_x x_{it-1} + e_{y,it}\\\\ x_{it} &amp; = \\alpha_{x} + \\beta_{1}x_{it-1} + \\omega_x y_{it-1} + e_{x,it}\\\\ e_{y,it} &amp; = s_i + u_{y,it}\\\\ e_{x,it} &amp; = s_i + u_{x,it}\\\\ \\end{matrix} \\] Setting aside the cross-lagged coefficient, i.e, \\(\\omega_x = 0\\), if one fits the standard CLPM without accounting for \\(s_i\\), the estimates will be biased. \\[ \\begin{matrix} y_{it} &amp; = \\alpha_{y} + \\beta_{y}y_{it-1} + e_{y,it}\\\\ x_{it} &amp; = \\alpha_{x} + \\beta_{x}x_{it-1} + e_{x,it}\\\\ \\end{matrix} \\] In this model, \\(\\hat\\beta\\) corresponds to \\[ \\begin{matrix} \\hat\\beta_y &amp;=&amp; {cov(y_{it}, y_{it-1}) \\over var(y_{it-1})}\\\\ &amp;=&amp; {cov(\\beta y_{it-1} + s_i + u_{y,it}, y_{it-1}) \\over var(y_{it-1})}\\\\ &amp;=&amp; \\frac{cov(\\beta_y y_{it-1}, y_{it-1}) + cov(s_i, y_{it-1}) + cov(u_{y,it}, y_{it-1})}{var(y_{it-1})}\\\\ (\\text{Exogeneity}) &amp;=&amp; \\frac{\\beta_y \\cdot var(y_{it-1}) + cov(s_i, y_{it-1})}{var(y_{it-1})}\\\\ &amp;=&amp; \\beta_y + \\frac{cov(s_i, y_{it-1})}{var(y_{it-1})}\\\\ \\end{matrix} \\] The stable unit level effect, \\(s_i\\) – invariant across time – is part of the error. Unless it is uncorrelated with \\(y_{it-1}\\), \\(\\hat\\beta_y\\) will be biased. Conditioning on \\(s_i\\) – including a fixed or random effect for each unit – attempts to remove this source of bias. For instance, the Random Intercept Cross Lagged Regression Model models “within” versus “between” person effects. A stable, trait level component – invariant across time – is modeled alongside “within” person fluctuations around this stable estimate. 1.6 The Random Intercept Cross Lagged Regression The RICLPM – a popular approach in psychology – also explicitly includes trait level variation, but in a manner somewhat different from the difference score approaches described. For the moment, ignoring the lags and cross-lagged effects, let us assume that \\(x\\) and \\(y\\) are each a function of a stable trait level component, plus a time-varying state component. The RICLPM is an extension of the CLPM, \\[ \\begin{matrix} y_{it} &amp; = \\alpha_{y} + \\beta_{y}y_{it-1} + e_{y,it}\\\\ x_{it} &amp; = \\alpha_{x} + \\beta_{x}x_{it-1} + e_{x,it}\\\\ \\end{matrix} \\] But the RICLPM includes a stable trait level component, \\(\\mu_i\\), for each individual \\(i\\): \\[ \\begin{matrix} y_{it} = \\mu^y_{i} + y^*_{y,it}\\\\ x_{it} = \\mu^x_{i} + x^*_{x,it}\\\\ \\end{matrix} \\] The \\(\\mu^x_i\\) and \\(\\mu^y_i\\) terms are the stable, trait level components for individual \\(i\\). And \\(x^*_{t,i}\\) and \\(y^*_{t,i}\\) terms are the time-varying state components. We can view this formation as providing a decomposition of the total variance in \\(x\\) and \\(y\\) into between-person (trait; \\(\\mu\\)) and within-person (state, \\(x^*,y^*\\)) components. From here, we can rewrite the cross-lagged and lagged effects from the state components alone. \\[ \\begin{matrix} y^*_{it} &amp; = \\beta_{y}y^*_{it-1} + \\omega_{y}x^*_{it-1} + e^*_{y,it}\\\\ x^*_{it} &amp; = \\beta_{x}x^*_{it-1} + \\omega_{x}y^*_{it-1} + e^*_{x,it}\\\\ \\end{matrix} \\] Unlike the CLPM, the RICLPM examines residualized change, or change net of stable trait level differences between individuals. The “dynamic” part of the model captures whether fluctation around the respondent’s mean on \\(x\\) relates to fluctuations around the respondent’s mean on \\(y\\) at the next time point, and vice versa. The RICLPM thus focuses on within-person effects – \\(y^*_{it}\\) and \\(x^*_{it}\\) represent deviations from the individual’s mean score for \\(y\\) and \\(x\\). Figure 1.3: Random Intercept Cross-Lagged Panel Model with 5 waves. The model shows autoregressive paths (solid arrows), cross-lagged effects (solid arrows), within-time correlations (dashed curved arrows), and residual terms (self-loops). As shown in Figure 1.3, the RI-CLPM separates between-person and within-person effects. The next section builds on these two models, using synthetic data, to illustrate the similarities and similarities between these approaches. 1.7 The Latent Growth Model The Latent Growth Model (LGM) provides another approach. As opposed to modeling autoregressive and cross-lagged effects directly, the LGCM models change over time using latent intercept and slope factors. The model imposes a particular type of growth on all units – linear, quadratic, exponential – but otherwise growth trajectories can be estimated for each unit. Figure 1.4: Latent Growth Model, Linear Trend. What’s particularly useful about this approach is that one can model individual differences in growth trajectories. This is indicated by the triangle shaped intercept term on the left – the model allows one to estimate unit values on all intercept and slope factors.The correlations between the latent growth factors, the intercepts \\(I\\) and slopes, \\(S\\) for both \\(x\\) and \\(y\\) capture unique relationships. \\(cor(S_X, I_X)\\): Captures whether individuals with higher initial levels of \\(x\\) also tend to have more rapid growth in \\(x\\) over time. \\(cor(S_Y, I_Y)\\): Captures whether individuals with higher initial levels of \\(y\\) also tend to have more rapid growth in \\(Y\\) over time. \\(cor(S_X, S_Y)\\): Captures whether individuals who experience more rapid growth in \\(x\\) also experience more rapid growth in \\(y\\) over time. \\(cor(I_X, I_Y)\\): Captures whether individuals with higher initial values (i.e., wave 1) of \\(x\\) also have higher starting values on \\(y\\). \\(cor(S_X, I_Y)\\): Captures whether individuals with higher initial values for \\(y\\) tend to experience more growth in \\(x\\) over time. \\(cor(S_Y, I_X)\\): Captures whether individuals with higher initial values for \\(x\\) tend to experience more growth in \\(y\\) over time. The latent growth model requires a minimum of two waves to estimate linear growth, and three or more waves to estimate quadratic growth. 1.8 Measurement Error Using latent variable structural equation models to estimate panel data relationships, whether that be change, growth, autoregression, and cross-lagged effects, allows researchers to remove measurement error by specifying multiple-indicator latent variables (Bollen 1989; Sullivan and Feldman 1979). While less effective than latent variable modelling, averaging multiple items to create scales can also be effective at reducing measurement error (Ansolabehere, Rodden, and Snyder 2008). Measurement error can pose serious challenges to correctly obtaining cross-lagged parameters from CLPMs and change score models. Moreover, the resulting bias is not random. Instead, it will systematically inflate the size of the cross-lagged parameter estimates and increase the risk of false positives. In the next section, these models are estimated and compared using synthetic data. References Abramowitz, Alan I. 1978. “The Impact of a Presidential Debate on Voter Rationality.” American Journal of Political Science 22 (3): 680–90. https://doi.org/10.2307/2110467. Allison, Paul D. 1990. “Change Scores as Dependent Variables in Regression Analysis.” Sociological Methodology, 93–114. Ansolabehere, Stephen, Jonathan Rodden, and James M. Snyder. 2008. “The Strength of Issues: Using Multiple Measures to Gauge Preference Stability, Ideological Constraint, and Issue Voting.” American Political Science Review 102 (2): 215–32. https://doi.org/10.1017/S0003055408080210. Bollen, Kenneth A. 1989. Structural Equations with Latent Variables. Structural Equations with Latent Variables. Oxford, England: John Wiley &amp; Sons. Campbell, James E., and Kenneth John Meier. 1979. “Style Issues and Vote Choice.” Political Behavior 1 (3): 203–15. https://doi.org/10.1007/BF00990588. Chiu, Albert, Xingchen Lan, Ziyi Liu, and Yiqing Xu. 2025. “Causal Panel Analysis Under Parallel Trends: Lessons from A Large Reanalysis Study.” American Political Science Review. https://doi.org/10.48550/arXiv.2309.15983. Grimm, Kevin J, Yang An, John J McArdle, Alan B Zonderman, and Susan M Resnick. 2012. “Recent Changes Leading to Subsequent Changes: Extensions of Multivariate Latent Difference Score Models.” Structural Equation Modeling: A Multidisciplinary Journal 19 (2): 268–92. Hamaker, Ellen L., Rebecca M. Kuiper, and Raoul P. P. P. Grasman. 2015. “A Critique of the Cross-Lagged Panel Model.” Psychological Methods 20 (1): 102–16. https://doi.org/10.1037/a0038889. Hsiao, Cheng. 2022. Analysis of Panel Data. 64. Cambridge university press. Iyengar, Shanto. 1978. “Testing the Transfer of Affect Hypothesis in a New Nation Using Panel Data.” American Journal of Political Science 22 (4): 905–16. https://doi.org/10.2307/2110598. Jennings, M. Kent, and Gregory B. Markus. 1977. “The Effect of Military Service on Political Attitudes: A Panel Study.” American Political Science Review 71 (1): 131–47. https://doi.org/10.2307/1956958. Jennings, M. Kent, and Richard G. Niemi. 1974. Political Character of Adolescence: The Influence of Families and Schools. Princeton University Press. Kenny, David A. 1975. “Cross-Lagged Panel Correlation: A Test for Spuriousness.” Psychological Bulletin 82 (6): 887. Kim, Yongnam, and Peter M Steiner. 2021. “Gain Scores Revisited: A Graphical Models Perspective.” Sociological Methods &amp; Research 50 (3): 1353–75. Klopack, Eric T, and Kandauda Wickrama. 2020. “Modeling Latent Change Score Analysis and Extensions in Mplus: A Practical Guide for Researchers.” Structural Equation Modeling: A Multidisciplinary Journal 27 (1): 97–110. Lüdtke, Oliver, and Alexander Robitzsch. 2022. “A Comparison of Different Approaches for Estimating Cross-Lagged Effects from a Causal Inference Perspective.” Structural Equation Modeling: A Multidisciplinary Journal 29 (6): 888–907. Markus, Gregory B. 1979a. Analyzing Panel Data: An Introduction. Edited by Michael S. Lewis-Beck. Quantitative Applications in the Social Sciences 18. Newbury Park, CA: Sage Publications. ———. 1979b. “The Political Environment and the Dynamics of Public Attitudes: A Panel Study.” American Journal of Political Science 23 (2): 338–59. https://doi.org/10.2307/2111006. Markus, Gregory B., and Philip E. Converse. 1979. “A Dynamic Simultaneous Equation Model of Electoral Choice.” American Political Science Review 73 (4): 1055–70. https://doi.org/10.2307/1953989. McArdle, John J. 2009. “Latent Variable Modeling of Differences and Changes with Longitudinal Data.” Annual Review of Psychology 60 (1): 577–605. McArdle, John J, and John R Nesselroade. 2003. “Growth Curve Analysis in Contemporary Psychological Research.” Handbook of Psychology, 447–80. McCullough, B. Claire. 1978. “Effects of Variables Using Panel Data: A Review of Techniques.” The Public Opinion Quarterly 42 (2): 199–220. Meier, Kenneth J., and James E. Campbell. 1979. “Issue Voting: An Empirical Examination of Individually Necessary and Jointly Sufficient Conditions.” American Politics Quarterly 7 (1): 21–50. https://doi.org/10.1177/1532673X7900700102. Shingles, Richard D. 1976. “Causal Inference in Cross-Lagged Panel Analysis.” Political Methodology 3 (1): 95–133. Stanley, Julian C, and Donald T Campbell. 1963. Experimental and Quasi-Experimental Designs for Research. Chicago: R. McNally. Sullivan, John L., and Stanley Feldman. 1979. Multiple Indicators: An Introduction. Edited by John L. Sullivan. Quantitative Applications in the Social Sciences 15. Beverly Hills, CA: Sage Publications. Usami, Satoshi. 2021. “On the Differences Between General Cross-Lagged Panel Model and Random-Intercept Cross-Lagged Panel Model: Interpretation of Cross-Lagged Parameters and Model Choice.” Structural Equation Modeling: A Multidisciplinary Journal 28 (3): 331–44. Usami, Satoshi, Timothy Hayes, and John J McArdle. 2015. “On the Mathematical Relationship Between Latent Change Score and Autoregressive Cross-Lagged Factor Approaches: Cautions for Inferring Causal Relationship Between Variables.” Multivariate Behavioral Research 50 (6): 676–87. "],["simple-motivating-examples.html", "Chapter 2 Simple Motivating Examples 2.1 Simulation 2.2 A Simple Example 2.3 Summary: Change versus Level Scores 2.4 Unobserved Heterogeneity 2.5 Model Validation", " Chapter 2 Simple Motivating Examples The package that accompanies this summary, crossLagR, includes functions to estimate several panel data models. They’re structured such that functions that begin with estimate are simple functions to write latent variable panel models. The (working) package can be accessed on github: devtools::install_github(\"crweber9874/crossLagR\") Below I use four functions, with simulated data, which is expanded to real panel data in the next section. estimateCLPM() generates the syntax for cross-lagged panel models. Use help(estimateRICLPM) to modify constraints and number of waves. estimateRICLPM() generates the syntax for random-intercept cross-lagged panel models. Use help(estimateRICLPM). estimateLChange() generates the syntax for latent change score model. Use help(estimateLChange). estimateLGM() generates the syntax for latent growth models. Use help(estimateLGM). Nearly all the functions in the package start with a core lavaan model – such as the RICLPM (Hamaker, Kuiper, and Grasman 2015). From there, build in rules to implement constraints, extend the number of waves, and modify parameters. You could just print the model code and modify manually. For instance, the lavaan code to estimate the CLPM is, estimateCLPM(waves = 2, constrain_beta = TRUE, constrain_omega = TRUE, constrain_residual_variances = TRUE, constrain_residual_covariances = TRUE, estimate_means = FALSE) |&gt; cat() ## p1 =~ 1*y1 ## q1 =~ 1*x1 ## p2 =~ 1*y2 ## q2 =~ 1*x2 ## p1 ~ 0*1 ## q1 ~ 0*1 ## p2 ~ 0*1 ## q2 ~ 0*1 ## y1 ~ 0*1 ## x1 ~0*1 ## y2 ~ 0*1 ## x2 ~0*1 ## p2 ~ beta_y*p1 + omega_xy*q1 ## q2 ~ beta_x*q1 + omega_yx*p1 ## p1 ~~ var_y1*p1 ## q1 ~~ var_x1*q1 ## p2 ~~ var_y*p2 ## q2 ~~ var_x*q2 ## p1 ~~ cov_xy1*q1 ## p2 ~~ cov_xy*q2 ## y1 ~~ 0*y1 ## x1 ~~ 0*x1 ## y2 ~~ 0*y2 ## x2 ~~ 0*x2 By modifying constrain_beta and constrain_omega, this frees the AR (former) or CL (latter) parameters to vary across waves. The lavaan code to estimate the Bivariate Latent Change Model is, estimateLChange(waves = 3, variable_type = &quot;bivariate&quot;, constrain_omega = TRUE, constrain_beta = TRUE) |&gt; cat() ## cf_x1 =~ 1*x1 ## cf_x2 =~ 1*x2 ## cf_x3 =~ 1*x3 ## cf_x1 ~ indicator_mean_x*1 ## cf_x2 ~ 0*1 ## cf_x3 ~ 0*1 ## cf_x1 ~~ start(15)*latent_indicator_mean_x*cf_x1 ## cf_x2 ~~ 0*cf_x2 ## cf_x3 ~~ 0*cf_x3 ## x1 ~ 0*1 ## x2 ~ 0*1 ## x3 ~ 0*1 ## x1 ~~ indicator_variance_x*x1 ## x2 ~~ indicator_variance_x*x2 ## x3 ~~ indicator_variance_x*x3 ## cf_x2 ~ 1*cf_x1 ## cf_x3 ~ 1*cf_x2 ## ld_x2 =~ 1*cf_x2 ## ld_x3 =~ 1*cf_x3 ## ld_x2 ~ 0*1 ## ld_x3 ~ 0*1 ## ld_x2 ~~ 0*ld_x2 ## ld_x3 ~~ 0*ld_x3 ## general_x =~ 1*ld_x2 ## + 1*ld_x3 ## general_x ~ constant_mean_x*1 ## general_x ~~ latent_variance_x*general_x ## general_x ~~ cf_x1 ## ld_x2 ~ start(-0.15)*beta_x*cf_x1 ## ld_x3 ~ start(-0.15)*beta_x*cf_x2 ## cf_y1 =~ 1*y1 ## cf_y2 =~ 1*y2 ## cf_y3 =~ 1*y3 ## cf_y1 ~ indicator_mean_y*1 ## cf_y2 ~ 0*1 ## cf_y3 ~ 0*1 ## cf_y1 ~~ start(15)*latent_variance_y*cf_y1 ## cf_y2 ~~ 0*cf_y2 ## cf_y3 ~~ 0*cf_y3 ## y1 ~ 0*1 ## y2 ~ 0*1 ## y3 ~ 0*1 ## y1 ~~ indicator_variance_y*y1 ## y2 ~~ indicator_variance_y*y2 ## y3 ~~ indicator_variance_y*y3 ## cf_y2 ~ 1*cf_y1 ## cf_y3 ~ 1*cf_y2 ## ld_y2 =~ 1*cf_y2 ## ld_y3 =~ 1*cf_y3 ## ld_y2 ~ 0*1 ## ld_y3 ~ 0*1 ## ld_y2 ~~ 0*ld_y2 ## ld_y3 ~~ 0*ld_y3 ## general_y =~ 1*ld_y2 ## + 1*ld_y3 ## general_y ~ constant_mean_y*1 ## general_y ~~ latent_variance_y*general_y ## general_y ~~ cf_y1 ## ld_y2 ~ beta_y*cf_y1 ## ld_y3 ~ beta_y*cf_y2 ## ld_x2 ~ start(-0.2)*omega_x*cf_y1 ## ld_y2 ~ start(-0.2)*omega_y*cf_x1 ## ld_x3 ~ start(-0.2)*omega_x*cf_y2 ## ld_y3 ~ start(-0.2)*omega_y*cf_x2 ## general_x ~~ general_y ## cf_x1 ~~ cf_y1 ## cf_x1 ~~ general_y ## cf_y1 ~~ general_x And the RICLPM model is, estimateRICLPM(waves = 3, constrain_omega = TRUE, constrain_beta = TRUE) |&gt; cat() ## ## # Random Intercepts ## BX =~ 1*x1 + 1*x2 + 1*x3 ## BY =~ 1*y1 + 1*y2 + 1*y3 ## ## # Within-Person Latent Variables ## p1 =~ 1*x1 ## q1 =~ 1*y1 ## p2 =~ 1*x2 ## q2 =~ 1*y2 ## p3 =~ 1*x3 ## q3 =~ 1*y3 ## ## # Latent Variable Means (Fixed to 0) ## p1 ~ 0*1 ## q1 ~ 0*1 ## ## p2 ~ 0*1 ## q2 ~ 0*1 ## ## p3 ~ 0*1 ## q3 ~ 0*1 ## ## # Autoregressive and Cross-lagged Effects ## p2 ~ beta_y*p1 + omega_xy*q1 ## q2 ~ beta_x*q1 + omega_yx*p1 ## p3 ~ beta_y*p2 + omega_xy*q2 ## q3 ~ beta_x*q2 + omega_yx*p2 ## ## # Residual Variances ## p1 ~~ var_p*p1 ## q1 ~~ var_q*q1 ## p2 ~~ var_p*p2 ## q2 ~~ var_q*q2 ## p3 ~~ var_p*p3 ## q3 ~~ var_q*q3 ## ## # Residual Covariances ## p1 ~~ cov_pq*q1 ## p2 ~~ cov_pq*q2 ## p3 ~~ cov_pq*q3 ## ## # Random Intercept Variances and Covariances ## BX ~~ BX ## BY ~~ BY ## BX ~~ BY ## ## # Fix Observed Variable Residuals to Zero ## x1 ~~ 0*x1 ## y1 ~~ 0*y1 ## x2 ~~ 0*x2 ## y2 ~~ 0*y2 ## x3 ~~ 0*x3 ## y3 ~~ 0*y3 And finally, the Bivariate Latent (Linear) Growth Model, estimateLGM(waves = 3, variable_type = &quot;bivariate&quot; ) |&gt; cat() ## # Latent Variables ## p1 =~ 1*x1 ## q1 =~ 1*y1 ## p2 =~ 1*x2 ## q2 =~ 1*y2 ## p3 =~ 1*x3 ## q3 =~ 1*y3 ## ## # Latent Growth Factors for X ## I_x =~ 1*p1 + 1*p2 + 1*p3 ## S_x =~ 0*p1 + 1*p2 + 2*p3 ## ## # Latent Growth Factors for Y ## I_y =~ 1*q1 + 1*q2 + 1*q3 ## S_y =~ 0*q1 + 1*q2 + 2*q3 ## ## # Growth Factor Means ## I_x ~ mean_I_x*1 ## S_x ~ mean_S_x*1 ## I_y ~ mean_I_y*1 ## S_y ~ mean_S_y*1 ## ## # Growth Factor Variances ## I_x ~~ var_I_x*I_x ## S_x ~~ var_S_x*S_x ## I_y ~~ var_I_y*I_y ## S_y ~~ var_S_y*S_y ## ## # Within-Variable Growth Factor Covariances ## I_x ~~ cov_IS_x*S_x ## I_y ~~ cov_IS_y*S_y ## ## # Between-Variable Growth Factor Covariances ## I_x ~~ cov_I_xy*I_y ## S_x ~~ cov_S_xy*S_y ## I_x ~~ cov_IS_xy*S_y ## I_y ~~ cov_IS_yx*S_x ## ## # Latent Variable Means (Fixed to 0) ## p1 ~ 0*1 ## q1 ~ 0*1 ## p2 ~ 0*1 ## q2 ~ 0*1 ## p3 ~ 0*1 ## q3 ~ 0*1 ## ## # Observed Variable Intercepts (Fixed to 0) ## x1 ~ 0*1 ## y1 ~ 0*1 ## x2 ~ 0*1 ## y2 ~ 0*1 ## x3 ~ 0*1 ## y3 ~ 0*1 ## ## # Latent Variable Variances ## p1 ~~ var_p*p1 ## q1 ~~ var_q*q1 ## p2 ~~ var_p*p2 ## q2 ~~ var_q*q2 ## p3 ~~ var_p*p3 ## q3 ~~ var_q*q3 ## ## # Latent Variable Covariances ## p1 ~~ cov_pq*q1 ## p2 ~~ cov_pq*q2 ## p3 ~~ cov_pq*q3 ## ## # Fix Observed Variable Residuals to Zero ## x1 ~~ 0*x1 ## y1 ~~ 0*y1 ## x2 ~~ 0*x2 ## y2 ~~ 0*y2 ## x3 ~~ 0*x3 ## y3 ~~ 0*y3 2.1 Simulation Each estimation function comes with an associated data simulation function. The functions simulate data according to the model specified. These are later used to build Monte Carlo functions. simCLPM() simulates data from a standard cross-lagged panel model. simRICLPM() simulates data from a random-intercept cross-lagged panel model. simLChange() simulates data from a latent change score model. simLGM() simulates data from a latent growth model. Documentation is included for each of the functions and can be accessed with the help() or ? function. 2.2 A Simple Example As a motivating example, let’s simulate data from a CLPM and estimate two regression models, one with AR and CL parameters in “level form” and another regression model on the change scores, also with AR and CL parameters. This is similar to Allison (1990)’s discussion of difference scores versus level scores.The regression models are just OLS models, one using change scores, the other using lags. Not surprisingly, and consistent with the previous section, they are interchangeable. Figure 2.1 shows the DGP and Figure 2.2 shows simulated data. Figure 2.1: The Cross-Lagged Panel Model with Multiple Indicators Figure 2.2: Data Generating Process: CLPM; Simulated trajectories shown Using these data, let’s estimate a change score regression, alongside a level score regression (estimating the CLPM with least squares). These are not latent variable models, but rather simple OLS regressions on observed data. ## ## Call: ## lm(formula = delta_y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6832 -0.6593 -0.0105 0.6663 3.3551 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.009138 0.015956 0.573 0.567 ## y_lag -0.625505 0.015482 -40.402 &lt;2e-16 *** ## x_lag 0.191603 0.014834 12.916 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9823 on 3997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.2914, Adjusted R-squared: 0.2911 ## F-statistic: 821.9 on 2 and 3997 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6832 -0.6593 -0.0105 0.6663 3.3551 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.009138 0.015956 0.573 0.567 ## y_lag 0.374495 0.015482 24.189 &lt;2e-16 *** ## x_lag 0.191603 0.014834 12.916 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9823 on 3997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.2289, Adjusted R-squared: 0.2285 ## F-statistic: 593.3 on 2 and 3997 DF, p-value: &lt; 2.2e-16 If we simulate data under a latent change model, with no constant change, the CLPM and difference parameters should align (they do). Notice that the \\(beta\\) parameter in the CLPM formulation is exactly the same as the difference score interpretation, as \\(\\hat\\beta_{CLPM} - 1 = \\hat{\\beta_\\Delta}\\), where \\(\\hat\\Delta\\) is the the difference score autoregressive parameter, and \\(\\hat\\beta_{CLPM}\\) is the level form autoregressive parameter. Likewise, we could simulate data under a latent change score specification (with no constant effects), and again the models mirror the DGP parameters and are related. ## ## Call: ## lm(formula = delta_y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4905 -0.7104 -0.0013 0.7202 4.2900 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.007258 0.005291 1.372 0.17 ## y_lag -0.737898 0.004695 -157.151 &lt;2e-16 *** ## x_lag 0.350068 0.004694 74.582 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.058 on 39997 degrees of freedom ## (10000 observations deleted due to missingness) ## Multiple R-squared: 0.3836, Adjusted R-squared: 0.3836 ## F-statistic: 1.245e+04 on 2 and 39997 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = y ~ y_lag + x_lag, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4905 -0.7104 -0.0013 0.7202 4.2900 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.007258 0.005291 1.372 0.17 ## y_lag 0.262102 0.004695 55.820 &lt;2e-16 *** ## x_lag 0.350068 0.004694 74.582 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.058 on 39997 degrees of freedom ## (10000 observations deleted due to missingness) ## Multiple R-squared: 0.2608, Adjusted R-squared: 0.2608 ## F-statistic: 7057 on 2 and 39997 DF, p-value: &lt; 2.2e-16 The estimateCLPM() function may be used to estimate a latent variable CLPM. Notice that if we simulate data under the latent change model (without constant change), the CLPM retrieves the correct estimates, \\(\\hat\\beta_{CLPM} - 1 \\approx - -0.7\\) ## lavaan 0.6-20 ended normally after 42 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 33 ## Number of equality constraints 21 ## ## Number of observations 10000 ## ## Model Test User Model: ## ## Test statistic 915.247 ## Degrees of freedom 53 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 =~ ## y1 1.000 ## q1 =~ ## x1 1.000 ## p2 =~ ## y2 1.000 ## q2 =~ ## x2 1.000 ## p3 =~ ## y3 1.000 ## q3 =~ ## x3 1.000 ## p4 =~ ## y4 1.000 ## q4 =~ ## x4 1.000 ## p5 =~ ## y5 1.000 ## q5 =~ ## x5 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## p2 ~ ## p1 (bt_y) 0.257 0.005 54.675 0.000 ## q1 (omg_x) 0.353 0.005 74.922 0.000 ## q2 ~ ## q1 (bt_x) 0.254 0.005 53.951 0.000 ## p1 (omg_y) 0.351 0.005 74.866 0.000 ## p3 ~ ## p2 (bt_y) 0.257 0.005 54.675 0.000 ## q2 (omg_x) 0.353 0.005 74.922 0.000 ## q3 ~ ## q2 (bt_x) 0.254 0.005 53.951 0.000 ## p2 (omg_y) 0.351 0.005 74.866 0.000 ## p4 ~ ## p3 (bt_y) 0.257 0.005 54.675 0.000 ## q3 (omg_x) 0.353 0.005 74.922 0.000 ## q4 ~ ## q3 (bt_x) 0.254 0.005 53.951 0.000 ## p3 (omg_y) 0.351 0.005 74.866 0.000 ## p5 ~ ## p4 (bt_y) 0.257 0.005 54.675 0.000 ## q4 (omg_x) 0.353 0.005 74.922 0.000 ## q5 ~ ## q4 (bt_x) 0.254 0.005 53.951 0.000 ## p4 (omg_y) 0.351 0.005 74.866 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 ~~ ## q1 (cv_1) 0.308 0.015 19.965 0.000 ## .p2 ~~ ## .q2 (cv_x) 0.321 0.006 55.181 0.000 ## .p3 ~~ ## .q3 (cv_x) 0.321 0.006 55.181 0.000 ## .p4 ~~ ## .q4 (cv_x) 0.321 0.006 55.181 0.000 ## .p5 ~~ ## .q5 (cv_x) 0.321 0.006 55.181 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 -0.027 0.012 -2.234 0.025 ## q1 0.010 0.012 0.795 0.427 ## .p2 0.000 ## .q2 0.000 ## .p3 0.000 ## .q3 0.000 ## .p4 0.000 ## .q4 0.000 ## .p5 0.000 ## .q5 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 (vr_y1) 1.510 0.021 70.711 0.000 ## q1 (vr_x1) 1.512 0.021 70.711 0.000 ## .p2 (vr_y) 1.120 0.008 141.421 0.000 ## .q2 (vr_x) 1.115 0.008 141.421 0.000 ## .p3 (vr_y) 1.120 0.008 141.421 0.000 ## .q3 (vr_x) 1.115 0.008 141.421 0.000 ## .p4 (vr_y) 1.120 0.008 141.421 0.000 ## .q4 (vr_x) 1.115 0.008 141.421 0.000 ## .p5 (vr_y) 1.120 0.008 141.421 0.000 ## .q5 (vr_x) 1.115 0.008 141.421 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 2.3 Summary: Change versus Level Scores Consistent with Allison (1990), there is a similarity between these two approaches – modeling dynamic processes in change and level form. However, the simulated examples thus far ignore an important dynamic, unobserved heterogeneity or unit effects. Hsiao (2022) shows that when autoregressive parameters are specified, and unit effects are ignored, the CLPM specification will yield biased parameter estimates, overestimating the autoregressive parameter and under-estimating the cross-lagged parameter. In the next section, we more fully consider the empirical consequences of unobserved heterogeneity and how latent variable models may be specified to estimated these random effects. 2.4 Unobserved Heterogeneity The CLPM and Change Score models estimated will produce biased parameter estimates, as unit effects are not considered. Both the RI-CLPM and the Latent Change Score model (LCM) address these concerns, by decomposing the variance in \\(y\\) to between (i.e., unit) and within (i.e., variation around the unit’s mean) in the case of the RI-CLPM versus a model that models constant change (i.e., unit change) and proportional change in the case of the LCM. Let’s begin by considering how the CLPM does not correctly retrieve parameter estimates assuming a LCM DGP with constant effects. The DGP is a Latent Change Model with Constant Effects and the Estimator is the CLPM Assume the DGP is a latent change model with constant effects. The data are generated following 2.3. Next, we estimate an ordinary CLPM. The autoregressive parameter (\\(\\hat\\beta \\approx 0.3\\)) is biased and is \\(\\approx 0.7\\). The cross lagged parameters (\\(\\hat \\omega \\approx 0.4\\)) where they should be closer to 0.5. Figure 2.3: DGP: Latent Change with Unit Effects; Estimator: CLPM. ## In this example, the DGP is a latent change model ## with constant effects. The estimator is a CLPM model, ## which doesn&#39;t fully control for unit effects. ## lavaan results are printed below ## lavaan 0.6-20 ended normally after 45 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 33 ## Number of equality constraints 21 ## ## Number of observations 10000 ## ## Model Test User Model: ## ## Test statistic 22100.637 ## Degrees of freedom 53 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 =~ ## y1 1.000 ## q1 =~ ## x1 1.000 ## p2 =~ ## y2 1.000 ## q2 =~ ## x2 1.000 ## p3 =~ ## y3 1.000 ## q3 =~ ## x3 1.000 ## p4 =~ ## y4 1.000 ## q4 =~ ## x4 1.000 ## p5 =~ ## y5 1.000 ## q5 =~ ## x5 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## p2 ~ ## p1 (bt_y) 0.708 0.004 160.670 0.000 ## q1 (omg_x) 0.435 0.004 99.028 0.000 ## q2 ~ ## q1 (bt_x) 0.710 0.004 161.454 0.000 ## p1 (omg_y) 0.436 0.004 98.890 0.000 ## p3 ~ ## p2 (bt_y) 0.708 0.004 160.670 0.000 ## q2 (omg_x) 0.435 0.004 99.028 0.000 ## q3 ~ ## q2 (bt_x) 0.710 0.004 161.454 0.000 ## p2 (omg_y) 0.436 0.004 98.890 0.000 ## p4 ~ ## p3 (bt_y) 0.708 0.004 160.670 0.000 ## q3 (omg_x) 0.435 0.004 99.028 0.000 ## q4 ~ ## q3 (bt_x) 0.710 0.004 161.454 0.000 ## p3 (omg_y) 0.436 0.004 98.890 0.000 ## p5 ~ ## p4 (bt_y) 0.708 0.004 160.670 0.000 ## q4 (omg_x) 0.435 0.004 99.028 0.000 ## q5 ~ ## q4 (bt_x) 0.710 0.004 161.454 0.000 ## p4 (omg_y) 0.436 0.004 98.890 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 ~~ ## q1 (cv_1) 0.314 0.015 20.398 0.000 ## .p2 ~~ ## .q2 (cv_x) 0.469 0.008 61.681 0.000 ## .p3 ~~ ## .q3 (cv_x) 0.469 0.008 61.681 0.000 ## .p4 ~~ ## .q4 (cv_x) 0.469 0.008 61.681 0.000 ## .p5 ~~ ## .q5 (cv_x) 0.469 0.008 61.681 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 -0.006 0.012 -0.450 0.652 ## q1 -0.000 0.012 -0.036 0.971 ## .p2 0.000 ## .q2 0.000 ## .p3 0.000 ## .q3 0.000 ## .p4 0.000 ## .q4 0.000 ## .p5 0.000 ## .q5 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 (vr_y1) 1.507 0.021 70.711 0.000 ## q1 (vr_x1) 1.512 0.021 70.711 0.000 ## .p2 (vr_y) 1.446 0.010 141.421 0.000 ## .q2 (vr_x) 1.450 0.010 141.421 0.000 ## .p3 (vr_y) 1.446 0.010 141.421 0.000 ## .q3 (vr_x) 1.450 0.010 141.421 0.000 ## .p4 (vr_y) 1.446 0.010 141.421 0.000 ## .q4 (vr_x) 1.450 0.010 141.421 0.000 ## .p5 (vr_y) 1.446 0.010 141.421 0.000 ## .q5 (vr_x) 1.450 0.010 141.421 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 What about the RI-CLPM? If we simulate data under the RI-CLPM DGP, does the CLPM retrieve the correct parameters? ## In this example, the DGP is the RICLPM ## The estimator is a CLPM model, ## which doesn&#39;t fully control for unit effects. ## lavaan results are printed below ## lavaan 0.6-20 ended normally after 41 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 33 ## Number of equality constraints 21 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 222.906 ## Degrees of freedom 53 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 =~ ## y1 1.000 ## q1 =~ ## x1 1.000 ## p2 =~ ## y2 1.000 ## q2 =~ ## x2 1.000 ## p3 =~ ## y3 1.000 ## q3 =~ ## x3 1.000 ## p4 =~ ## y4 1.000 ## q4 =~ ## x4 1.000 ## p5 =~ ## y5 1.000 ## q5 =~ ## x5 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## p2 ~ ## p1 (bt_y) 0.433 0.015 29.797 0.000 ## q1 (omg_x) 0.204 0.015 13.991 0.000 ## q2 ~ ## q1 (bt_x) 0.410 0.015 27.897 0.000 ## p1 (omg_y) 0.235 0.015 16.056 0.000 ## p3 ~ ## p2 (bt_y) 0.433 0.015 29.797 0.000 ## q2 (omg_x) 0.204 0.015 13.991 0.000 ## q3 ~ ## q2 (bt_x) 0.410 0.015 27.897 0.000 ## p2 (omg_y) 0.235 0.015 16.056 0.000 ## p4 ~ ## p3 (bt_y) 0.433 0.015 29.797 0.000 ## q3 (omg_x) 0.204 0.015 13.991 0.000 ## q4 ~ ## q3 (bt_x) 0.410 0.015 27.897 0.000 ## p3 (omg_y) 0.235 0.015 16.056 0.000 ## p5 ~ ## p4 (bt_y) 0.433 0.015 29.797 0.000 ## q4 (omg_x) 0.204 0.015 13.991 0.000 ## q5 ~ ## q4 (bt_x) 0.410 0.015 27.897 0.000 ## p4 (omg_y) 0.235 0.015 16.056 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 ~~ ## q1 (cv_1) 0.651 0.051 12.695 0.000 ## .p2 ~~ ## .q2 (cv_x) 0.200 0.018 11.389 0.000 ## .p3 ~~ ## .q3 (cv_x) 0.200 0.018 11.389 0.000 ## .p4 ~~ ## .q4 (cv_x) 0.200 0.018 11.389 0.000 ## .p5 ~~ ## .q5 (cv_x) 0.200 0.018 11.389 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 0.038 0.039 0.973 0.330 ## q1 0.073 0.038 1.922 0.055 ## .p2 0.000 ## .q2 0.000 ## .p3 0.000 ## .q3 0.000 ## .p4 0.000 ## .q4 0.000 ## .p5 0.000 ## .q5 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## p1 (vr_y1) 1.530 0.068 22.361 0.000 ## q1 (vr_x1) 1.442 0.064 22.361 0.000 ## .p2 (vr_y) 1.087 0.024 44.721 0.000 ## .q2 (vr_x) 1.101 0.025 44.721 0.000 ## .p3 (vr_y) 1.087 0.024 44.721 0.000 ## .q3 (vr_x) 1.101 0.025 44.721 0.000 ## .p4 (vr_y) 1.087 0.024 44.721 0.000 ## .q4 (vr_x) 1.101 0.025 44.721 0.000 ## .p5 (vr_y) 1.087 0.024 44.721 0.000 ## .q5 (vr_x) 1.101 0.025 44.721 0.000 ## .y1 0.000 ## .x1 0.000 ## .y2 0.000 ## .x2 0.000 ## .y3 0.000 ## .x3 0.000 ## .y4 0.000 ## .x4 0.000 ## .y5 0.000 ## .x5 0.000 Notice, the same problem exists. The model does a poor job retrieving the correct parameter estimates. Why does the CLPM get it wrong? It only does as the proportion of the variance in \\(y\\) (or \\(x\\)) increases due to unit effect – unobserved heterogeneity. The CLPM assumes that all variance is wave-specific, and thus misattributes stable variance in \\(y\\) to the autoregression parameters. This can be seen in examining the simulated trajectories of data generated varying the variance parameters for the random intercepts. First, let’s consider a situation in which the between variance is 4 times as large as the within variance. Figure 2.4: Individual trajectories under RICLPM This can be compared to the same DGP but where the within variance is 4 times as large as the between variance. library(dplyr) library(ggplot2) library(crossLagR) library(lavaan) wide_data = simRICLPM( waves = 5, beta_x = 0.3, beta_y = 0.8, sample.nobs = 1000, var_BX = 0.25, var_BY = 0.25, var_p = 1, var_q = 1, estimate_means = TRUE)$data ## Warning: lavaan-&gt;lav_start_check_cov(): ## starting values imply a correlation larger than 1; variables involved are: ## BX BY ## ✅ Successfully simulated RICLPM data with 5 waves and 1000 observations. data = wide_data |&gt; reshape_long_sim_cr() |&gt; group_by(id) |&gt; mutate( y_lag = lag(y), x_lag = lag(x), delta_y = y - lag(y) ) |&gt; ungroup() -&gt; data data |&gt; ggplot(aes(x = wave, y = y, group = id)) + geom_line(alpha = 0.1, color = &quot;black&quot;, position = position_jitter(width = 0.1, height = 0.2)) + stat_summary(aes(group = 1), fun = mean, geom = &quot;line&quot;, color = &quot;lightblue&quot;, size = 1, alpha = 0.4) + stat_summary(aes(group = 1), fun = mean, geom = &quot;point&quot;, color = &quot;lightblue&quot;, size = 1) + labs( title = &quot;Individual Trajectories: Simulated Data&quot;, subtitle = &quot;Within Variance = 4x as large as Between&quot;, x = &quot;Survey Wave&quot;, y = &quot;Response&quot; ) + theme_minimal() + scale_y_continuous(limits = c(-5, 6) ) + theme(strip.text = element_text(color = &quot;black&quot;)) -&gt; plot print(plot) Figure 2.5: Individual trajectories under RICLPM Figure 2.4 shows real differences between units – they’re spread out; whereas Figure 2.5 shows much less difference between units – they’re more tightly compact. The fundamental relationship between the points over time does not change, all that changes is the spread of the trajectories. 2.5 Model Validation It may be useful to show these models can be estimated using techniques more familiar to a political science audience. Below, I estimate both the latent change model with constant effects and the RI-CLPM using brms. 2.5.1 Hierarchical brms Estimation This is the differenced variable regression with random intercepts. summary(model) ## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be ## careful when analysing the results! We recommend running more iterations and/or ## setting stronger priors. ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: delta_y ~ -1 + y_lag + x_lag + (1 | id) ## delta_x ~ -1 + y_lag + x_lag + (1 | id) ## Data: data (Number of observations: 29000) ## Draws: 3 chains, each with iter = 1000; warmup = 500; thin = 1; ## total post-warmup draws = 1500 ## ## Multilevel Hyperparameters: ## ~id (Number of levels: 1000) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## sd(deltay_Intercept) 1.74 0.04 1.66 1.82 1.07 42 ## sd(deltax_Intercept) 1.73 0.04 1.65 1.82 1.07 43 ## Tail_ESS ## sd(deltay_Intercept) 145 ## sd(deltax_Intercept) 72 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## deltay_y_lag -0.84 0.01 -0.85 -0.83 1.00 2356 1150 ## deltay_x_lag 0.20 0.01 0.19 0.21 1.00 911 1029 ## deltax_y_lag 0.18 0.01 0.17 0.19 1.00 970 1182 ## deltax_x_lag -0.84 0.01 -0.85 -0.83 1.01 3186 1226 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_deltay 1.07 0.00 1.06 1.08 1.00 2884 1118 ## sigma_deltax 1.05 0.00 1.04 1.06 1.00 3465 1316 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## rescor(deltay,deltax) 0.11 0.01 0.10 0.12 1.01 3702 ## Tail_ESS ## rescor(deltay,deltax) 871 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). References Allison, Paul D. 1990. “Change Scores as Dependent Variables in Regression Analysis.” Sociological Methodology, 93–114. Hamaker, Ellen L., Rebecca M. Kuiper, and Raoul P. P. P. Grasman. 2015. “A Critique of the Cross-Lagged Panel Model.” Psychological Methods 20 (1): 102–16. https://doi.org/10.1037/a0038889. Hsiao, Cheng. 2022. Analysis of Panel Data. 64. Cambridge university press. "],["short-t-bias..html", "Chapter 3 Short t bias. 3.1 The Fixed Effect (Dynamic Panel) Estimator 3.2 The RICLPM 3.3 Change 3.4 Change 3.5 Summary", " Chapter 3 Short t bias. It is well established that estimating a dynamic fixed effects model with short t – relatively few waves – can lead to biased estimates. This is shown in Hsiao (2022). To see this, here is a dynamic panel data simulation where the DGP is an RI-CLPM with known parameters. 3.1 The Fixed Effect (Dynamic Panel) Estimator This example simulates data from a four wave panel. Notice that the parameter estimates are far from their true values. library(crossLagR) library(dplyr) library(purrr) library(tidyr) # Long data baseRICLPM_sim(n = 1000, ar_x = 0.5, ar_y = 0.5, cl_xy = 0.3, cl_yx = 0.2, waves = 4) |&gt; select(-contains(&quot;trait&quot;)) |&gt; # reshape to long reshape_long_sim_cr() -&gt; dat lm(within.y ~ xlagw + ylagw, data = dat) |&gt; summary() ## ## Call: ## lm(formula = within.y ~ xlagw + ylagw, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.96282 -0.36166 0.00648 0.35937 1.88265 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0005934 0.0096297 0.062 0.951 ## xlagw 0.1446878 0.0184855 7.827 6.87e-15 *** ## ylagw -0.1454482 0.0184051 -7.903 3.80e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5274 on 2997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.03394, Adjusted R-squared: 0.0333 ## F-statistic: 52.65 on 2 and 2997 DF, p-value: &lt; 2.2e-16 With more waves \\(t = 30\\), the model recovers the true parameters. #| echo: true #| message: false #| warning: false # Long data library(crossLagR) library(dplyr) library(purrr) library(tidyr) baseRICLPM_sim(n = 1000, ar_x = 0.5, ar_y = 0.5, cl_xy = 0.3, cl_yx = 0.2, waves = 30) |&gt; select(-contains(&quot;trait&quot;)) |&gt; # reshape to long reshape_long_sim_cr() -&gt; dat lm(within.y ~ xlagw + ylagw, data = dat) |&gt; summary() ## ## Call: ## lm(formula = within.y ~ xlagw + ylagw, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.71943 -0.46828 0.00234 0.46715 3.11892 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.0007708 0.0040732 -0.189 0.85 ## xlagw 0.2872604 0.0056747 50.621 &lt;2e-16 *** ## ylagw 0.4545222 0.0053462 85.018 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6936 on 28997 degrees of freedom ## (1000 observations deleted due to missingness) ## Multiple R-squared: 0.4006, Adjusted R-squared: 0.4006 ## F-statistic: 9690 on 2 and 28997 DF, p-value: &lt; 2.2e-16 3.2 The RICLPM The RICLPM doesn’t exhibit the same problem, with minimal bias (that decreases) with more waves. The code below runs a simple Monte Carlo analysis varying the number of panel waves, and the following fixed parameters (\\(n = 2000; \\beta_x = 0.4; \\beta_y = 0.7; \\omega_{x \\rightarrow y} = 0.2; \\omega_{y \\rightarrow x} = 0.3\\)). library(crossLagR) library(dplyr) library(purrr) library(ggplot2) library(tidyr) true_params &lt;- list(beta_x = 0.4, beta_y = 0.7, omega_xy = 0.2, omega_yx = 0.3) extract_params &lt;- function(fit) { pe &lt;- lavaan::parameterEstimates(fit) factor_scores &lt;- lavaan::lavPredict(fit, method = &quot;EBM&quot;) factor_scores_df &lt;- as_tibble(factor_scores) %&gt;% select(starts_with(&quot;general_y&quot;) | starts_with(&quot;general_x&quot;)) tibble( beta_x = pe$est[pe$label == &quot;beta_x&quot;][1], beta_y = pe$est[pe$label == &quot;beta_y&quot;][1], omega_xy = pe$est[pe$label == &quot;omega_xy&quot;][1], omega_yx = pe$est[pe$label == &quot;omega_yx&quot;][1], ) } results &lt;- expand_grid( waves = c(3, 5, 7, 10, 30), rep = 1:50 ) |&gt; mutate( data = map(waves, ~{ baseRICLPM_sim( n = 2000, innov_var = 0.45, ar_x = 0.4, ar_y = 0.7, cl_xy = 0.3, cl_yx = 0.2, waves = .x ) |&gt; select(-contains(&quot;trait&quot;)) }), # This iterates over the data, x and waves y fit = map2(data, waves, ~{ lavaan::lavaan(estimateRICLPM(waves = ..2, label_autoregressive = c(&quot;beta_x&quot;, &quot;beta_y&quot;)), data = ..1, warn = FALSE, verbose = FALSE) }), params = map(fit, extract_params) ) |&gt; unnest(params) library(ggridges) library(dplyr) library(tidyr) library(ggplot2) library(ggridges) # Prepare data with simpler labels for parsing results_long &lt;- results |&gt; select(waves, rep, beta_x, beta_y, omega_xy, omega_yx) |&gt; pivot_longer(cols = c(beta_x, beta_y, omega_xy, omega_yx), names_to = &quot;parameter&quot;, values_to = &quot;estimate&quot;) |&gt; mutate( parameter_label = case_when( parameter == &quot;beta_x&quot; ~ &quot;Autoregressive[x]&quot;, parameter == &quot;beta_y&quot; ~ &quot;Autoregressive[y]&quot;, parameter == &quot;omega_xy&quot; ~ &quot;CrossLag~X%-&gt;%Y~(omega[xy])&quot;, parameter == &quot;omega_yx&quot; ~ &quot;CrossLag~Y%-&gt;%X~(omega[yx])&quot; ), true_value = case_when( parameter == &quot;beta_x&quot; ~ 0.4, parameter == &quot;beta_y&quot; ~ 0.7, parameter == &quot;omega_xy&quot; ~ 0.2, parameter == &quot;omega_yx&quot; ~ 0.3 ), waves_factor = factor(waves, levels = c(30, 10, 7, 5, 3)) ) # Calculate medians for each wave/parameter combination medians &lt;- results_long |&gt; group_by(waves_factor, parameter_label, true_value) |&gt; summarise(median_est = median(estimate), .groups = &quot;drop&quot;) # Create plot with median lines ggplot(results_long, aes(x = estimate, y = waves_factor)) + geom_density_ridges( aes(fill = waves_factor), scale = 2, rel_min_height = 0.01, alpha = 0.7 ) + geom_vline(aes(xintercept = true_value), linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 0.8, alpha = 0.5) + geom_segment(data = medians, aes(x = median_est, xend = median_est, y = as.numeric(waves_factor) - 0.3, yend = as.numeric(waves_factor) + 0.3), color = &quot;green&quot;, size = 1) + facet_wrap(~parameter_label, scales = &quot;free_x&quot;, labeller = label_parsed, nrow = 2) + scale_fill_viridis_d(option = &quot;viridis&quot;, begin = 0.2, end = 0.9) + labs( title = &quot;Monte Carlo Sample Distribution Varying Waves, RI-CLPM&quot;, subtitle = &quot;Green line = median estimate | Dashed line = true value&quot;, x = &quot;Parameter Estimate&quot;, y = &quot;Number of Waves&quot; ) + theme_minimal(base_size = 12) + theme( legend.position = &quot;none&quot;, strip.text = element_text(size = 11, face = &quot;bold&quot;), panel.grid.minor = element_blank() ) ## Picking joint bandwidth of 0.00646 ## Picking joint bandwidth of 0.00468 ## Picking joint bandwidth of 0.00435 ## Picking joint bandwidth of 0.00561 Here is a summary table, library(gt) bias_summary &lt;- results |&gt; group_by(waves) |&gt; summarise( across(c(beta_x, beta_y, omega_xy, omega_yx), list( mean = mean, bias = ~mean(.) - true_params[[cur_column()]], relative_bias = ~ (mean(.) - true_params[[cur_column()]]) / true_params[[cur_column()]]*100, se = ~sd(.) ), .names = &quot;{.col}_{.fn}&quot;) ) |&gt; select(waves, contains(&quot;bias&quot;)) |&gt; mutate(across(where(is.numeric), ~round(., 3))) bias_summary |&gt; gt() |&gt; tab_header( title = md(&quot;**Monte Carlo Bias Analysis**&quot;), subtitle = &quot;RICLPM Estimation: Varying the Number of Waves&quot; ) |&gt; cols_label( waves = &quot;Waves&quot;, beta_x_bias = &quot;Bias, AR(X)&quot;, beta_y_bias = &quot;Bias, AR(Y)&quot;, beta_x_relative_bias = &quot;Relative Bias AR(X) %&quot;, beta_y_relative_bias = &quot;Relative Bias AR(Y) %&quot;, omega_xy_bias = &quot;Bias, CL(X→Y)&quot;, omega_yx_bias = &quot;Bias, CL(Y→X)&quot;, omega_xy_relative_bias = &quot;Relative Bias, CL(X→Y) %&quot;, omega_yx_relative_bias = &quot;Relative Bias, CL(Y→X) %&quot; )|&gt; tab_source_note( source_note = md(&quot;*Note*: The results reported here are from 100 simulations, varying the number of waves.&quot;) ) |&gt; tab_footnote( footnote = &quot;Bias = Mean(estimate) - True value, Relative Bias = (Bias / True value) * 100&quot;) #fdwpofwsdv table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #fdwpofwsdv thead, #fdwpofwsdv tbody, #fdwpofwsdv tfoot, #fdwpofwsdv tr, #fdwpofwsdv td, #fdwpofwsdv th { border-style: none; } #fdwpofwsdv p { margin: 0; padding: 0; } #fdwpofwsdv .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #fdwpofwsdv .gt_caption { padding-top: 4px; padding-bottom: 4px; } #fdwpofwsdv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #fdwpofwsdv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #fdwpofwsdv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fdwpofwsdv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fdwpofwsdv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fdwpofwsdv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #fdwpofwsdv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #fdwpofwsdv .gt_column_spanner_outer:first-child { padding-left: 0; } #fdwpofwsdv .gt_column_spanner_outer:last-child { padding-right: 0; } #fdwpofwsdv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #fdwpofwsdv .gt_spanner_row { border-bottom-style: hidden; } #fdwpofwsdv .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #fdwpofwsdv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #fdwpofwsdv .gt_from_md > :first-child { margin-top: 0; } #fdwpofwsdv .gt_from_md > :last-child { margin-bottom: 0; } #fdwpofwsdv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #fdwpofwsdv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #fdwpofwsdv .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #fdwpofwsdv .gt_row_group_first td { border-top-width: 2px; } #fdwpofwsdv .gt_row_group_first th { border-top-width: 2px; } #fdwpofwsdv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fdwpofwsdv .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #fdwpofwsdv .gt_first_summary_row.thick { border-top-width: 2px; } #fdwpofwsdv .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fdwpofwsdv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fdwpofwsdv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #fdwpofwsdv .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #fdwpofwsdv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #fdwpofwsdv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fdwpofwsdv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fdwpofwsdv .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #fdwpofwsdv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fdwpofwsdv .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #fdwpofwsdv .gt_left { text-align: left; } #fdwpofwsdv .gt_center { text-align: center; } #fdwpofwsdv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #fdwpofwsdv .gt_font_normal { font-weight: normal; } #fdwpofwsdv .gt_font_bold { font-weight: bold; } #fdwpofwsdv .gt_font_italic { font-style: italic; } #fdwpofwsdv .gt_super { font-size: 65%; } #fdwpofwsdv .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #fdwpofwsdv .gt_asterisk { font-size: 100%; vertical-align: 0; } #fdwpofwsdv .gt_indent_1 { text-indent: 5px; } #fdwpofwsdv .gt_indent_2 { text-indent: 10px; } #fdwpofwsdv .gt_indent_3 { text-indent: 15px; } #fdwpofwsdv .gt_indent_4 { text-indent: 20px; } #fdwpofwsdv .gt_indent_5 { text-indent: 25px; } #fdwpofwsdv .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #fdwpofwsdv div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Monte Carlo Bias Analysis RICLPM Estimation: Varying the Number of Waves Waves Bias, AR(X) Relative Bias AR(X) % Bias, AR(Y) Relative Bias AR(Y) % Bias, CL(X→Y) Relative Bias, CL(X→Y) % Bias, CL(Y→X) Relative Bias, CL(Y→X) % 3 0.001 0.287 0.002 0.318 -0.007 -3.282 -0.006 -1.876 5 0.000 0.030 -0.003 -0.371 0.001 0.509 0.002 0.516 7 0.000 -0.075 -0.003 -0.445 -0.001 -0.512 0.002 0.516 10 0.000 -0.016 0.001 0.091 0.001 0.466 0.000 -0.053 30 0.000 -0.038 0.000 0.049 0.000 -0.038 0.000 0.087 Bias = Mean(estimate) - True value, Relative Bias = (Bias / True value) * 100 Note: The results reported here are from 100 simulations, varying the number of waves. 3.3 Change Below I attempt to do the same analysis but with the latent change score model with constant effects. In these simulations, I vary the degree of “between-unit” and “within-wave” variation. I didn’t have much luck here….strange results. library(dplyr) library(purrr) library(tidyr) baseRICLPM_sim(n = 1000, ar_x = 0.4, ar_y = 0.7, cl_xy = 0.3, cl_yx = 0.2, waves = 10) |&gt; select(-contains(&quot;trait&quot;)) -&gt; dat fit = lavaan::lavaan(estimateLChange(waves = 10, variable_type = &quot;bivariate&quot;), data = dat, warn = FALSE, verbose = FALSE) true_params &lt;- list(beta_x = -0.6 , beta_y = -0.3, omega_x = 0.2, omega_y = 0.3) extract_params &lt;- function(fit) { pe &lt;- lavaan::parameterEstimates(fit) tibble( beta_x = pe$est[pe$label == &quot;beta_x&quot;][1], beta_y = pe$est[pe$label == &quot;beta_y&quot;][1], omega_x = pe$est[pe$label == &quot;omega_x&quot;][1], omega_y = pe$est[pe$label == &quot;omega_y&quot;][1] ) } results &lt;- expand_grid( waves = c(3, 5, 10, 30), innov_var = seq(0.25, 1.25, by = 1), rep = 1:50 ) |&gt; mutate( data = pmap(list(waves = waves, innov_var = innov_var), ~{ baseRICLPM_sim( n = 2000, innov_var = ..2, # second arg pmap ar_x = 0.4, ar_y = 0.7, cl_xy = 0.3, cl_yx = 0.2, waves = ..1 # first arg in pmap ) }), # This iterates over the data, x and waves y fit = map2(data, waves, ~{ lavaan::lavaan(estimateLChange(waves = ..2, variable_type = &quot;bivariate&quot;, estimate_constant_change = FALSE), data = ..1, warn = FALSE, verbose = FALSE) }), params = map(fit, extract_params) ) |&gt; unnest(params) results |&gt; mutate( beta_y = beta_y , beta_x = beta_x ) |&gt; group_by(waves, innov_var) |&gt; summarise( across(c(beta_x, beta_y, omega_y, omega_x), list( mean = mean, bias = ~mean(.) - true_params[[cur_column()]], relative_bias = ~ (mean(.) - true_params[[cur_column()]]) / true_params[[cur_column()]]*100, se = ~sd(.) ), .names = &quot;{.col}_{.fn}&quot;)) -&gt; bias_summary 3.4 Change library(gt) bias_summary |&gt; select(waves, contains(c(&quot;mean&quot;, &quot;waves&quot;, &quot;innov_var&quot;))) |&gt; mutate(across(where(is.numeric), ~round(., 3))) ## # A tibble: 8 × 6 ## # Groups: waves [4] ## waves beta_x_mean beta_y_mean omega_y_mean omega_x_mean innov_var ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 -0.036 -0.046 0.057 0.014 0.25 ## 2 3 -0.172 -0.169 0.21 0.065 1.25 ## 3 5 -0.024 -0.048 0.044 -0.001 0.25 ## 4 5 -0.136 -0.188 0.203 0.032 1.25 ## 5 10 -0.008 -0.057 0.033 -0.016 0.25 ## 6 10 -0.099 -0.194 0.187 0.003 1.25 ## 7 30 0.001 -0.059 0.017 -0.025 0.25 ## 8 30 -0.054 -0.182 0.154 -0.024 1.25 3.5 Summary The change (LCM) and level score (CLPM; RICLPM) analysis are equivalent in the absence of unit effects. This suggests a close relationship between these models. However, when unit effects are present, the models diverge. When the DGP includes unit effects, the same interchangeable relationships disappear. References Hsiao, Cheng. 2022. Analysis of Panel Data. 64. Cambridge university press. "],["outline.html", "Chapter 4 Outline", " Chapter 4 Outline I. Introduction/Outline A. Critique of the CLPM Fails to consider unit effects Confounds lag with unit effects The CLPM can be written with lags or as a change score regression Here we can use much of the work Adam finished on the earlier draft. Alternative estimators B. Level Score Models Fixed Effects Estimation with lagged effects (Dynamic Panel Model). Biased in short t panels Random Intercept Cross-Lagged Panel Model (RI-CLPM) Requires at least three waves of data Separates between and within unit effects Estimates represent within-unit changes from average scores Computational limitations: Inefficient in short t panels C. Change Score Models Latent Change Score Model (LCSM) Models change processes directly and explicitly controls for unit effects With two waves, first difference regression With 3 waves, proportional change. How much does \\(y\\) yesterday effect a change in \\(y\\). With 4+ waves, dual change; constant change and proportional change. Simulation/Monte Carlo Analysis A. Simulate data from a CLPM, a CLPM + Unit Effects (RICLPM), and LCM (with/without constant effects) B. Vary sample size, number of waves, and variance components (between/within variance) C. Estimate CLPM, CLPM+Fixed Effects, RICLPM, LCM, LCM+constant effects D. Highlight computational issues (convergence for RICLPM), bias and efficiency Applied Examples A. Short t panel. Stable constructs. ANES panel. Compare estimates. Personality and politics. B. Moderate t panel. LISS panel. Relatively stable constructs. Compare estimates. Personality and politics C. Moderate t panel, less stable constructs. 2012-2020. “Institute for the Study of Citizens and Politics Panel Study” (ISCP). Partisanship (stable) and political legitimacy (less stable, should fluctuate by partisanship based on president). “I would rather live under our system of government than any other that I can think of.” D. Long t panel Discussion/Conclusion Summary of findings Recommendations for applied researchers…models and data constraints – e.g., RICLPM is effective with marginally stable constructs and moderate to longer t panels. LCM models change processes explicitly. The CLPM almost always will get it wrong. Abramowitz, Alan I. 1978. “The Impact of a Presidential Debate on Voter Rationality.” American Journal of Political Science 22 (3): 680–90. https://doi.org/10.2307/2110467. Allison, Paul D. 1990. “Change Scores as Dependent Variables in Regression Analysis.” Sociological Methodology, 93–114. Ansolabehere, Stephen, Jonathan Rodden, and James M. Snyder. 2008. “The Strength of Issues: Using Multiple Measures to Gauge Preference Stability, Ideological Constraint, and Issue Voting.” American Political Science Review 102 (2): 215–32. https://doi.org/10.1017/S0003055408080210. Bollen, Kenneth A. 1989. Structural Equations with Latent Variables. Structural Equations with Latent Variables. Oxford, England: John Wiley &amp; Sons. Campbell, James E., and Kenneth John Meier. 1979. “Style Issues and Vote Choice.” Political Behavior 1 (3): 203–15. https://doi.org/10.1007/BF00990588. Chiu, Albert, Xingchen Lan, Ziyi Liu, and Yiqing Xu. 2025. “Causal Panel Analysis Under Parallel Trends: Lessons from A Large Reanalysis Study.” American Political Science Review. https://doi.org/10.48550/arXiv.2309.15983. Grimm, Kevin J, Yang An, John J McArdle, Alan B Zonderman, and Susan M Resnick. 2012. “Recent Changes Leading to Subsequent Changes: Extensions of Multivariate Latent Difference Score Models.” Structural Equation Modeling: A Multidisciplinary Journal 19 (2): 268–92. Hamaker, Ellen L., Rebecca M. Kuiper, and Raoul P. P. P. Grasman. 2015. “A Critique of the Cross-Lagged Panel Model.” Psychological Methods 20 (1): 102–16. https://doi.org/10.1037/a0038889. Hsiao, Cheng. 2022. Analysis of Panel Data. 64. Cambridge university press. Iyengar, Shanto. 1978. “Testing the Transfer of Affect Hypothesis in a New Nation Using Panel Data.” American Journal of Political Science 22 (4): 905–16. https://doi.org/10.2307/2110598. Jennings, M. Kent, and Gregory B. Markus. 1977. “The Effect of Military Service on Political Attitudes: A Panel Study.” American Political Science Review 71 (1): 131–47. https://doi.org/10.2307/1956958. Jennings, M. Kent, and Richard G. Niemi. 1974. Political Character of Adolescence: The Influence of Families and Schools. Princeton University Press. Kenny, David A. 1975. “Cross-Lagged Panel Correlation: A Test for Spuriousness.” Psychological Bulletin 82 (6): 887. Kim, Yongnam, and Peter M Steiner. 2021. “Gain Scores Revisited: A Graphical Models Perspective.” Sociological Methods &amp; Research 50 (3): 1353–75. Klopack, Eric T, and Kandauda Wickrama. 2020. “Modeling Latent Change Score Analysis and Extensions in Mplus: A Practical Guide for Researchers.” Structural Equation Modeling: A Multidisciplinary Journal 27 (1): 97–110. Lüdtke, Oliver, and Alexander Robitzsch. 2022. “A Comparison of Different Approaches for Estimating Cross-Lagged Effects from a Causal Inference Perspective.” Structural Equation Modeling: A Multidisciplinary Journal 29 (6): 888–907. Markus, Gregory B. 1979a. Analyzing Panel Data: An Introduction. Edited by Michael S. Lewis-Beck. Quantitative Applications in the Social Sciences 18. Newbury Park, CA: Sage Publications. ———. 1979b. “The Political Environment and the Dynamics of Public Attitudes: A Panel Study.” American Journal of Political Science 23 (2): 338–59. https://doi.org/10.2307/2111006. Markus, Gregory B., and Philip E. Converse. 1979. “A Dynamic Simultaneous Equation Model of Electoral Choice.” American Political Science Review 73 (4): 1055–70. https://doi.org/10.2307/1953989. McArdle, John J. 2009. “Latent Variable Modeling of Differences and Changes with Longitudinal Data.” Annual Review of Psychology 60 (1): 577–605. McArdle, John J, and John R Nesselroade. 2003. “Growth Curve Analysis in Contemporary Psychological Research.” Handbook of Psychology, 447–80. McCullough, B. Claire. 1978. “Effects of Variables Using Panel Data: A Review of Techniques.” The Public Opinion Quarterly 42 (2): 199–220. Meier, Kenneth J., and James E. Campbell. 1979. “Issue Voting: An Empirical Examination of Individually Necessary and Jointly Sufficient Conditions.” American Politics Quarterly 7 (1): 21–50. https://doi.org/10.1177/1532673X7900700102. Shingles, Richard D. 1976. “Causal Inference in Cross-Lagged Panel Analysis.” Political Methodology 3 (1): 95–133. Stanley, Julian C, and Donald T Campbell. 1963. Experimental and Quasi-Experimental Designs for Research. Chicago: R. McNally. Sullivan, John L., and Stanley Feldman. 1979. Multiple Indicators: An Introduction. Edited by John L. Sullivan. Quantitative Applications in the Social Sciences 15. Beverly Hills, CA: Sage Publications. Usami, Satoshi. 2021. “On the Differences Between General Cross-Lagged Panel Model and Random-Intercept Cross-Lagged Panel Model: Interpretation of Cross-Lagged Parameters and Model Choice.” Structural Equation Modeling: A Multidisciplinary Journal 28 (3): 331–44. Usami, Satoshi, Timothy Hayes, and John J McArdle. 2015. “On the Mathematical Relationship Between Latent Change Score and Autoregressive Cross-Lagged Factor Approaches: Cautions for Inferring Causal Relationship Between Variables.” Multivariate Behavioral Research 50 (6): 676–87. "]]

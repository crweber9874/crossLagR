---
output:
  pdf_document: default
  html_document: default
---
# Examples of Estimating Cross-Lagged Panel Models in R

The package that accompanies this summary, `crossLagR`, includes functions to estimate several panel data models. They're structured such that functions that begin with `estimate` are simple functions to write latent variable panel models. The (working) package can be accessed on `github`:  `devtools::install_github("crweber9874/crossLagR")`

```{r echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# devtools::install_github("crweber9874/crossLagR")
library(crossLagR)
library(brms)
library(dplyr)
library(ggplot2)
```

Below I use four functions, with simulated data, which is expanded to real panel data in the next section.

* `estimateCLPM()` generates the syntax for cross-lagged panel models. Use `help(estimateRICLPM)` to modify constraints and number of waves.
* `estimateRICLPM()` generates the syntax for random-intercept cross-lagged panel models. Use `help(estimateRICLPM)`.
* `estimateLChange()` generates the syntax for latent change score model. Use `help(estimateLChange)`.
* `estimateLGM()` generates the syntax for latent growth models. Use `help(estimateLGM)`.

Nearly all the functions in the package start with a core lavaan model -- such as the RICLPM [@hamaker_critique_2015]. From there, build in rules to implement constraints, extend the number of waves, and modify parameters. You could just print the model code and modify manually. For instance, the `lavaan` code to estimate the CLPM is,

```{r, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE }
estimateCLPM(waves = 2,
                         constrain_beta = TRUE,
                         constrain_omega = TRUE,
                         constrain_residual_variances = TRUE,
                         constrain_residual_covariances = TRUE,
                         estimate_means = FALSE) |>
cat()
```  

By modifying `constrain_beta` and `constrain_omega`, this frees the AR (former) or CL (latter) parameters to vary across waves. The `lavaan` code to estimate the **Bivariate Latent Change Model** is,

```{r, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE }
estimateLChange(waves = 3,
                variable_type = "bivariate",
                constrain_omega = TRUE,
                constrain_beta =  TRUE) |>
  cat()
```

And the **RICLPM** model is,

```{r, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE }
estimateRICLPM(waves = 3,
                constrain_omega = TRUE,
                constrain_beta =  TRUE) |>
  cat()
```
                       
And finally, the **Bivariate Latent (Linear) Growth Model**,

```{r, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE }
estimateLGM(waves = 3,
            variable_type = "bivariate"
) |>
cat()
```  

## Simulation

Each estimation function comes with an associated data simulation function. The functions simulate data according to the model specified. These are later used to build Monte Carlo functions.

* `simCLPM()` simulates data from a standard cross-lagged panel model.
* `simRICLPM()` simulates data from a random-intercept cross-lagged panel model.
* `simLChange()` simulates data from a latent change score model.
* `simLGM()` simulates data from a latent growth model.

Documentation is included for each of the functions and can be accessed with the `help()` or `?` function.

## A Simple Example

As a motivating example, let's simulate data from a CLPM and  estimate two regression models, one with AR and CL parameters in "level form" and another regression model on the change scores, also with AR and CL parameters. This is similar to @allison1990's discussion of difference scores versus level scores.The regression models are just OLS models, one using change scores, the other using lags. Not surprisingly, and consistent with the previous section, they are interchangeable. Figure \@ref(fig:latent-clpm) shows the DGP and Figure \@ref(fig:trajectories) shows simulated data.

```{r latent-clpm, echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, fig.cap="The Cross-Lagged Panel Model with Multiple Indicators"}
library(DiagrammeR)
clpm_diagram <- grViz("
digraph RICLPM {

  graph [layout = neato, rankdir = TB, bgcolor = white]

  node [shape = box, style = filled, fillcolor = white, fontsize = 10]

  # X indicators (3 per time point) - closer to latents
  x11 [pos = '-3.2,1.8!', label = 'x₁₁', width = 0.4, height = 0.25]
  x12 [pos = '-2.8,1.8!', label = 'x₁₂', width = 0.4, height = 0.25] 
  x13 [pos = '-2.4,1.8!', label = 'x₁₃', width = 0.4, height = 0.25]
  
  x21 [pos = '-1.7,1.8!', label = 'x₂₁', width = 0.4, height = 0.25]
  x22 [pos = '-1.3,1.8!', label = 'x₂₂', width = 0.4, height = 0.25]
  x23 [pos = '-0.9,1.8!', label = 'x₂₃', width = 0.4, height = 0.25]
  
  x31 [pos = '-0.2,1.8!', label = 'x₃₁', width = 0.4, height = 0.25]
  x32 [pos = '0.2,1.8!', label = 'x₃₂', width = 0.4, height = 0.25]
  x33 [pos = '0.6,1.8!', label = 'x₃₃', width = 0.4, height = 0.25]
  
  x41 [pos = '1.3,1.8!', label = 'x₄₁', width = 0.4, height = 0.25]
  x42 [pos = '1.7,1.8!', label = 'x₄₂', width = 0.4, height = 0.25]
  x43 [pos = '2.1,1.8!', label = 'x₄₃', width = 0.4, height = 0.25]
  
  x51 [pos = '2.8,1.8!', label = 'x₅₁', width = 0.4, height = 0.25]
  x52 [pos = '3.2,1.8!', label = 'x₅₂', width = 0.4, height = 0.25]
  x53 [pos = '3.6,1.8!', label = 'x₅₃', width = 0.4, height = 0.25]

  # Within Person X latents
  wX1 [pos = '-2.8,1!', shape = circle, fillcolor = white, label = 'q1']
  wX2 [pos = '-1.3,1!', shape = circle, fillcolor = white, label = 'q2']
  wX3 [pos = '0.2,1!', shape = circle, fillcolor = white, label = 'q3']
  wX4 [pos = '1.7,1!', shape = circle, fillcolor = white, label = 'q4']
  wX5 [pos = '3.2,1!', shape = circle, fillcolor = white, label = 'q5']

  # Within-person Y latents
  wY1 [pos = '-2.8,-1!', shape = circle, fillcolor = white, label = 'p1']
  wY2 [pos = '-1.3,-1!', shape = circle, fillcolor = white, label = 'p2']
  wY3 [pos = '0.2,-1!', shape = circle, fillcolor = white, label = 'p3']
  wY4 [pos = '1.7,-1!', shape = circle, fillcolor = white, label = 'p4']
  wY5 [pos = '3.2,-1!', shape = circle, fillcolor = white, label = 'p5']

  # Y indicators (3 per time point) - closer to latents
  y11 [pos = '-3.2,-1.8!', label = 'y₁₁', width = 0.4, height = 0.25]
  y12 [pos = '-2.8,-1.8!', label = 'y₁₂', width = 0.4, height = 0.25]
  y13 [pos = '-2.4,-1.8!', label = 'y₁₃', width = 0.4, height = 0.25]
  
  y21 [pos = '-1.7,-1.8!', label = 'y₂₁', width = 0.4, height = 0.25]
  y22 [pos = '-1.3,-1.8!', label = 'y₂₂', width = 0.4, height = 0.25]
  y23 [pos = '-0.9,-1.8!', label = 'y₂₃', width = 0.4, height = 0.25]
  
  y31 [pos = '-0.2,-1.8!', label = 'y₃₁', width = 0.4, height = 0.25]
  y32 [pos = '0.2,-1.8!', label = 'y₃₂', width = 0.4, height = 0.25]
  y33 [pos = '0.6,-1.8!', label = 'y₃₃', width = 0.4, height = 0.25]
  
  y41 [pos = '1.3,-1.8!', label = 'y₄₁', width = 0.4, height = 0.25]
  y42 [pos = '1.7,-1.8!', label = 'y₄₂', width = 0.4, height = 0.25]
  y43 [pos = '2.1,-1.8!', label = 'y₄₃', width = 0.4, height = 0.25]
  
  y51 [pos = '2.8,-1.8!', label = 'y₅₁', width = 0.4, height = 0.25]
  y52 [pos = '3.2,-1.8!', label = 'y₅₂', width = 0.4, height = 0.25]
  y53 [pos = '3.6,-1.8!', label = 'y₅₃', width = 0.4, height = 0.25]

  # Measurement model for X with proper subscripts
  wX1 -> x11 [label = '1', arrowsize = 0.4, fontsize = 8]
  wX1 -> x12 [label = <λ<sub>12</sub>>, arrowsize = 0.4, fontsize = 8]
  wX1 -> x13 [label = <λ<sub>13</sub>>, arrowsize = 0.4, fontsize = 8]
  
  wX2 -> x21 [label = '1', arrowsize = 0.4, fontsize = 8]
  wX2 -> x22 [label = <λ<sub>22</sub>>, arrowsize = 0.4, fontsize = 8]
  wX2 -> x23 [label = <λ<sub>23</sub>>, arrowsize = 0.4, fontsize = 8]
  
  wX3 -> x31 [label = '1', arrowsize = 0.4, fontsize = 8]
  wX3 -> x32 [label = <λ<sub>32</sub>>, arrowsize = 0.4, fontsize = 8]
  wX3 -> x33 [label = <λ<sub>33</sub>>, arrowsize = 0.4, fontsize = 8]
  
  wX4 -> x41 [label = '1', arrowsize = 0.4, fontsize = 8]
  wX4 -> x42 [label = <λ<sub>42</sub>>, arrowsize = 0.4, fontsize = 8]
  wX4 -> x43 [label = <λ<sub>43</sub>>, arrowsize = 0.4, fontsize = 8]
  
  wX5 -> x51 [label = '1', arrowsize = 0.4, fontsize = 8]
  wX5 -> x52 [label = <λ<sub>52</sub>>, arrowsize = 0.4, fontsize = 8]
  wX5 -> x53 [label = <λ<sub>53</sub>>, arrowsize = 0.4, fontsize = 8]

  # Measurement model for Y with proper subscripts
  wY1 -> y11 [label = '1', arrowsize = 0.4, fontsize = 8]
  wY1 -> y12 [label = <λ<sub>12</sub>>, arrowsize = 0.4, fontsize = 8]
  wY1 -> y13 [label = <λ<sub>13</sub>>, arrowsize = 0.4, fontsize = 8]
  
  wY2 -> y21 [label = '1', arrowsize = 0.4, fontsize = 8]
  wY2 -> y22 [label = <λ<sub>22</sub>>, arrowsize = 0.4, fontsize = 8]
  wY2 -> y23 [label = <λ<sub>23</sub>>, arrowsize = 0.4, fontsize = 8]
  
  wY3 -> y31 [label = '1', arrowsize = 0.4, fontsize = 8]
  wY3 -> y32 [label = <λ<sub>32</sub>>, arrowsize = 0.4, fontsize = 8]
  wY3 -> y33 [label = <λ<sub>33</sub>>, arrowsize = 0.4, fontsize = 8]
  
  wY4 -> y41 [label = '1', arrowsize = 0.4, fontsize = 8]
  wY4 -> y42 [label = <λ<sub>42</sub>>, arrowsize = 0.4, fontsize = 8]
  wY4 -> y43 [label = <λ<sub>43</sub>>, arrowsize = 0.4, fontsize = 8]
  
  wY5 -> y51 [label = '1', arrowsize = 0.4, fontsize = 8]
  wY5 -> y52 [label = <λ<sub>52</sub>>, arrowsize = 0.4, fontsize = 8]
  wY5 -> y53 [label = <λ<sub>53</sub>>, arrowsize = 0.4, fontsize = 8]

  # Autoregressive paths (β parameters) - fontsize 11
  wX1 -> wX2 [color = black, penwidth = 2, arrowsize = 0.5, label = 'β', fontsize = 11]
  wX2 -> wX3 [color = black, penwidth = 2, arrowsize = 0.5, label = 'β', fontsize = 11]
  wX3 -> wX4 [color = black, penwidth = 2, arrowsize = 0.5, label = 'β', fontsize = 11]
  wX4 -> wX5 [color = black, penwidth = 2, arrowsize = 0.5, label = 'β', fontsize = 11]
  wY1 -> wY2 [color = black, penwidth = 2, arrowsize = 0.5, label = 'β', fontsize = 11]
  wY2 -> wY3 [color = black, penwidth = 2, arrowsize = 0.5, label = 'β', fontsize = 11]
  wY3 -> wY4 [color = black, penwidth = 2, arrowsize = 0.5, label = 'β', fontsize = 11]
  wY4 -> wY5 [color = black, penwidth = 2, arrowsize = 0.5, label = 'β', fontsize = 11]

# Cross-lagged paths (ω parameters) - fontsize 11
  wX1 -> wY2 [color = black, penwidth = 2, arrowsize = 0.5, label = 'ω', fontsize = 11]
  wX2 -> wY3 [color = black, penwidth = 2, arrowsize = 0.5, label = 'ω', fontsize = 11]
  wX3 -> wY4 [color = black, penwidth = 2, arrowsize = 0.5, label = 'ω', fontsize = 11]
  wX4 -> wY5 [color = black, penwidth = 2, arrowsize = 0.5, label = 'ω', fontsize = 11]
  wY1 -> wX2 [color = black, penwidth = 2, arrowsize = 0.5, label = 'ω', fontsize = 11]
  wY2 -> wX3 [color = black, penwidth = 2, arrowsize = 0.5, label = 'ω', fontsize = 11]
  wY3 -> wX4 [color = black, penwidth = 2, arrowsize = 0.5, label = 'ω', fontsize = 11]
  wY4 -> wX5 [color = black, penwidth = 2, arrowsize = 0.5, label = 'ω', fontsize = 11]
  
  
  # Within-time correlations
  wX1 -> wY1 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5]
  wX2 -> wY2 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5]
  wX3 -> wY3 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5]
  wX4 -> wY4 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5]
  wX5 -> wY5 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5]

  # ERROR VARIANCES ON X INDICATORS (north side) - no labels
  x11 -> x11 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x12 -> x12 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x13 -> x13 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x21 -> x21 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x22 -> x22 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x23 -> x23 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x31 -> x31 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x32 -> x32 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x33 -> x33 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x41 -> x41 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x42 -> x42 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x43 -> x43 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x51 -> x51 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x52 -> x52 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]
  x53 -> x53 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n, arrowsize = 0.4]

  # ERROR VARIANCES ON Y INDICATORS (south side) - no labels
  y11 -> y11 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y12 -> y12 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y13 -> y13 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y21 -> y21 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y22 -> y22 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y23 -> y23 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y31 -> y31 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y32 -> y32 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y33 -> y33 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y41 -> y41 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y42 -> y42 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y43 -> y43 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y51 -> y51 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y52 -> y52 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]
  y53 -> y53 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.4]

  # Self-loops to represent errors on latent factors
  wX2 -> wX2 [dir = both, color = black, style = dashed, label = '', tailport = sw, headport = s, arrowsize = 0.5]
  wX3 -> wX3 [dir = both, color = black, style = dashed, label = '', tailport = sw, headport = s, arrowsize = 0.5]
  wX4 -> wX4 [dir = both, color = black, style = dashed, label = '', tailport = sw, headport = s, arrowsize = 0.5]
  wX5 -> wX5 [dir = both, color = black, style = dashed, label = '', tailport = sw, headport = s, arrowsize = 0.5]
  wY2 -> wY2 [dir = both, color = black, style = dashed, label = '', tailport = nw, headport = n, arrowsize = 0.5]
  wY3 -> wY3 [dir = both, color = black, style = dashed, label = '', tailport = nw, headport = n, arrowsize = 0.5]
  wY4 -> wY4 [dir = both, color = black, style = dashed, label = '', tailport = nw, headport = n, arrowsize = 0.5]
  wY5 -> wY5 [dir = both, color = black, style = dashed, label = '', tailport = nw, headport = n, arrowsize = 0.5]

}
")

clpm_diagram
```

```{r trajectories, fig.cap="Data Generating Process: CLPM; Simulated trajectories shown "}
#| echo: false
#| message: false
#| warning: false


library(dplyr)
library(ggplot2)
library(crossLagR)
library(lavaan)

wide_data = simCLPM(waves = 5,
                    mean_y1 = 0.3,
                    mean_x1 = -0.5)$data 

data = wide_data |>
  reshape_long_sim_cr() |>
  group_by(id) |>
  mutate(
    y_lag = lag(y),
    x_lag = lag(x),
    delta_y = y - lag(y)
  ) |>
  ungroup() -> data

data |>  
   ggplot(aes(x = wave, y = y, group = id)) + 
   geom_line(alpha = 0.1, color = "black", position = position_jitter(width = 0.1, height = 0.2)) +
   stat_summary(aes(group = 1), fun = mean, geom = "line",  
                color = "lightblue", size = 1, alpha = 0.4) + 
   stat_summary(aes(group = 1), fun = mean, geom = "point",  
                color = "lightblue", size = 1) + 
   labs(
     title = "Individual Trajectories: Simulated Data", 
     subtitle = "Data from Cross Lagged Panel Model", 
     x = "Survey Wave",
     y = "Response" 
   ) +
  theme_minimal() + 
  scale_y_continuous(limits = c(-5, 6) ) + 
  theme(strip.text = element_text(color = "black")) -> plot 
print(plot)

```

Using these data, let's estimate a change score regression, alongside a level score regression (estimating the CLPM with least squares). These are not latent variable models, but rather simple OLS regressions on observed data.

```{r, echo = FALSE, message = FALSE, error = FALSE}
lm(delta_y ~ y_lag + x_lag, data = data) |> summary()
lm(y ~ y_lag + x_lag, data = data) |> summary()
```
If we simulate data under a latent change model, with no constant change, the CLPM and difference parameters should align (they do). Notice that the $beta$ parameter in the CLPM formulation is exactly the same as the difference score interpretation, as $\hat\beta_{CLPM} - 1 = \hat{\beta_\Delta}$, where $\hat\Delta$ is the the difference score autoregressive parameter, and $\hat\beta_{CLPM}$ is the level form autoregressive parameter. Likewise, we could simulate data under a latent change score specification (with no constant effects), and again the models mirror the DGP parameters and are related.

```{r, echo = FALSE, message = FALSE, error = FALSE}
simLChange(
  waves = 5,
  variable_type = "bivariate",
  beta_x = -0.7,    # This should give CLPM beta ≈ 0.3
  beta_y = -0.7,    # This should give CLPM beta ≈ 0.3
  omega_x = 0.5,    # Cross-lagged coupling effects
  omega_y = 0.5,
  estimate_constant_change = FALSE,  # Important: no constant change
  sample.nobs = 10000
)$data |>
  reshape_long_sim_cr() |>
  group_by(id) |>
    mutate(
      y_lag = lag(y),
      x_lag = lag(x),
      delta_y = y - lag(y)
  ) |>
  ungroup() -> data

lm(delta_y ~ y_lag + x_lag, data = data) |> summary()
lm(y ~ y_lag + x_lag, data = data) |> summary()

```

The `estimateCLPM()` function may be used to estimate a latent variable CLPM. Notice that if we simulate data under the latent change model (without constant change), the CLPM retrieves the correct estimates, $\hat\beta_{CLPM} - 1 \approx -0.7$


```{r, echo = FALSE, message = FALSE, error = FALSE}
lcsm_data <- simLChange(
  waves = 5,
  variable_type = "bivariate",
  beta_x = -0.7,    # This should give CLPM beta ≈ 0.3
  beta_y = -0.7,    # This should give CLPM beta ≈ 0.3
  omega_x = 0.5,    # Cross-lagged coupling effects
  omega_y = 0.5,
  estimate_constant_change = FALSE,  # Important: no constant change
  sample.nobs = 10000
)
summary(lavaan(estimateCLPM(waves = 5), data = lcsm_data$data))
```

## Summary: Change versus Level Scores

Consistent with @allison1990, there is a similarity between these two approaches -- modeling dynamic processes in change and level form. However, the simulated examples thus far ignore an important dynamic, *unobserved heterogeneity* or *unit effects*. @hsiao2022 shows that when autoregressive parameters are specified, and unit effects are ignored, the CLPM specification will yield biased parameter estimates, overestimating the autoregressive parameter and under-estimating the cross-lagged parameter. In the next section, we more fully consider the empirical consequences of unobserved heterogeneity and how latent variable models may be specified to estimated these *random effects.* 

## Unobserved Heterogeneity

The CLPM and Change Score models estimated will produce biased parameter estimates, as unit effects are not considered. Both the RI-CLPM and the Latent Change Score model (LCM) address these concerns, by decomposing the variance in $y$ to **between** (i.e., unit) and **within** (i.e., variation around the unit's mean) in the case of the RI-CLPM versus a model that models **constant** change (i.e., unit change) and **proportional** change in the case of the LCM.

Let's begin by considering how the CLPM does not correctly retrieve parameter estimates assuming a LCM DGP with constant effects.


**The DGP is a Latent Change Model with Constant Effects and the Estimator is the CLPM**

Assume the DGP is a latent change model with constant effects.  The data are generated following \@ref(fig:lcm). Next, we estimate an ordinary CLPM. The autoregressive parameter ($\hat\beta \approx 0.3$) is biased and is $\approx 0.7$. The cross lagged parameters ($\hat \omega \approx 0.4$) where they should be closer to 0.5.

```{r lcm, echo = FALSE, message = FALSE, error = FALSE, fig.cap="DGP: Latent Change with Unit Effects; Estimator: CLPM."}
library(DiagrammeR)
lcm <- grViz("
digraph RICLPM {

  # Graph attributes
  graph [layout = neato, rankdir = TB, bgcolor = white]

  # Node attributes
  node [shape = box, style = filled, fillcolor = white, fontsize = 11]

  A_X [pos = '-2.5,1!', shape = circle, fillcolor = white, label = <Δ<sub>x</sub>>,  width = 0.5, height = 0.5]
  I [pos = '-2.5,0!', shape = triangle, fillcolor = white, label = 'I',  width = 0.5, height = 0.5]
  X1 [pos = '-2,2.2!', label = 'x1', width = 0.6, height = 0.3]
  X2 [pos = '-1,2.2!', label = 'x2', width = 0.6, height = 0.3]
  X3 [pos = '0,2.2!', label = 'x3', width = 0.6, height = 0.3]
  X4 [pos = '1,2.2!', label = 'x4', width = 0.6, height = 0.3]
  X5 [pos = '2,2.2!', label = 'x5', width = 0.6, height = 0.3]

  # Row 2: Within-person X latents (middle-upper)
  wX1 [pos = '-2,1.35!', shape = circle, fillcolor = white, label = 'q1']
  wX2 [pos = '-1,1.35!', shape = circle, fillcolor = white, label = 'q2']
  wX3 [pos = '0,1.35!', shape = circle, fillcolor = white, label = 'q3']
  wX4 [pos = '1,1.35!', shape = circle, fillcolor = white, label = 'q4']
  wX5 [pos = '2,1.35!', shape = circle, fillcolor = white, label = 'q5']

  # Row 3: Within-person Y latents (middle-lower)
  wY1 [pos = '-2,-1.35!', shape = circle, fillcolor = white, label = 'p1']
  wY2 [pos = '-1,-1.35!', shape = circle, fillcolor = white, label = 'p2']
  wY3 [pos = '0,-1.35!', shape = circle, fillcolor = white, label = 'p3']
  wY4 [pos = '1,-1.35!', shape = circle, fillcolor = white, label = 'p4']
  wY5 [pos = '2,-1.35!', shape = circle, fillcolor = white, label = 'p5']


  dX2 [pos = '-1,0.5!', shape = circle, fillcolor = white, label = 'Δx2']
  dX3 [pos = '0,0.5!', shape = circle, fillcolor = white, label = 'Δx3']
  dX4 [pos = '1,0.5!', shape = circle, fillcolor = white, label = 'Δx4']
  dX5 [pos = '2,0.5!', shape = circle, fillcolor = white, label = 'Δx5']

  dY2 [pos = '-1,-0.5!', shape = circle, fillcolor = white, label = 'Δy2']
  dY3 [pos = '0,-0.5!',  shape = circle, fillcolor = white, label = 'Δy3']
  dY4 [pos = '1,-0.5!',  shape = circle, fillcolor = white, label = 'Δy4']
  dY5 [pos = '2,-0.5!',  shape = circle, fillcolor = white, label = 'Δy5']



  # Row 4: observed Y variables and RI_Y (bottom)
  Y1 [pos = '-2,-2.2!', label = 'y1', width = 0.6, height = 0.3]
  Y2 [pos = '-1,-2.2!', label = 'y2', width = 0.6, height = 0.3]
  Y3 [pos = '0,-2.2!', label = 'y3', width = 0.6, height = 0.3]
  Y4 [pos = '1,-2.2!', label = 'y4', width = 0.6, height = 0.3]
  Y5 [pos = '2,-2.2!', label = 'y5', width = 0.6, height = 0.3]
  A_Y [pos = '-2.5,-1!', shape = circle, fillcolor = white, label = <Δ<sub>y</sub>>, width = 0.5, height = 0.5]


  # Measurement model paths (RI to observed)
  A_X -> dX2 [label = '', arrowsize = 0.5, tailport = e, headport = n]
  A_X -> dX3 [label = '',  arrowsize = 0.5, tailport = e, headport = n]
  A_X -> dX4 [label = '', arrowsize = 0.5, tailport = e, headport = n]
  A_X -> dX5 [label = '', arrowsize = 0.5, tailport = e, headport = n]

  A_Y -> dY2 [label = '', arrowsize = 0.5, tailport = e, headport = s]
  A_Y -> dY3 [label = '',  arrowsize = 0.5, tailport = e, headport = s]
  A_Y -> dY4 [label = '', arrowsize = 0.5, tailport = e, headport = s]
  A_Y -> dY5 [label = '', arrowsize = 0.5, tailport = e, headport = s]

  I -> A_X [label = '', arrowsize = 0.5, tailport = n, headport = s]
  I -> A_Y [label = '', arrowsize = 0.5, tailport = s, headport = n]
  I -> wX1 [label = '', arrowsize = 0.5, tailport = n, headport = s]
  I -> wY1 [label = '', arrowsize = 0.5, tailport = s, headport = n]

 
  # Measurement model paths (w to observed)
  wX1 -> X1 [label = '1', arrowsize = 0.5, fontsize = 11]
  wX2 -> X2 [label = '1', arrowsize = 0.5, fontsize = 11]
  wX3 -> X3 [label = '1', arrowsize = 0.5, fontsize = 11]
  wX4 -> X4 [label = '1', arrowsize = 0.5, fontsize = 11]
  wX5 -> X5 [label = '1', arrowsize = 0.5, fontsize = 11]
  wY1 -> Y1 [label = '1', arrowsize = 0.5, fontsize = 11]
  wY2 -> Y2 [label = '1', arrowsize = 0.5, fontsize = 11]
  wY3 -> Y3 [label = '1', arrowsize = 0.5, fontsize = 11]
  wY4 -> Y4 [label = '1', arrowsize = 0.5, fontsize = 11]
  wY5 -> Y5 [label = '1', arrowsize = 0.5, fontsize = 11]
  wY1 -> dY2 [label = 'β', arrowsize = 0.5, fontsize = 10]
  wY2 -> dY3 [label = 'β', arrowsize = 0.5, fontsize = 10]
  wY3 -> dY4 [label = 'β', arrowsize = 0.5, fontsize = 10]
  wY4 -> dY5 [label = 'β', arrowsize = 0.5, fontsize = 10]

  wX1 -> dX2 [label = 'β', arrowsize = 0.5, fontsize = 10]
  wX2 -> dX3 [label = 'β', arrowsize = 0.5, fontsize = 10]
  wX3 -> dX4 [label = 'β', arrowsize = 0.5, fontsize = 10]
  wX4 -> dX5 [label = 'β', arrowsize = 0.5, fontsize = 10]

  wX1 -> dY2 [label = 'θ', arrowsize = 0.5, fontsize = 10]
  wX2 -> dY3 [label = 'θ', arrowsize = 0.5, fontsize = 10]
  wX3 -> dY4 [label = 'θ', arrowsize = 0.5, fontsize = 10]
  wX4 -> dY5 [label = 'θ', arrowsize = 0.5, fontsize = 10]

  wY1 -> dX2 [label = 'θ', arrowsize = 0.5, fontsize = 10]
  wY2 -> dX3 [label = 'θ', arrowsize = 0.5, fontsize = 10]
  wY3 -> dX4 [label = 'θ', arrowsize = 0.5, fontsize = 10]
  wY4 -> dX5 [label = 'θ', arrowsize = 0.5, fontsize = 10]

  # Autoregressive paths
  wX1 -> wX2 [color = black, penwidth = 1, label = '1', arrowsize = 0.5, fontsize = 11]
  wX2 -> wX3 [color = black, penwidth = 1, label = '1', arrowsize = 0.5, fontsize = 11]
  wX3 -> wX4 [color = black, penwidth = 1, label = '1', arrowsize = 0.5, fontsize = 11]
  wX4 -> wX5 [color = black, penwidth = 1, label = '1', arrowsize = 0.5, fontsize = 11]
  wY1 -> wY2 [color = black, penwidth = 1, label = '1', arrowsize = 0.5, fontsize = 11]
  wY2 -> wY3 [color = black, penwidth = 1, label = '1', arrowsize = 0.5, fontsize = 11]
  wY3 -> wY4 [color = black, penwidth = 1, label = '1', arrowsize = 0.5, fontsize = 11]
  wY4 -> wY5 [color = black, penwidth = 1, label = '1', arrowsize = 0.5, fontsize = 11]

  # Cross-lagged paths with offset gamma labels and subscripts
  dX2 -> dY3 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]
  dX3 -> dY4 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]
  dX4 -> dY5 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]
  dY2 -> dX3 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]
  dY3 -> dX4 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]
  dY4 -> dX5 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]

  dX2 -> wX2 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]
  dX3 -> wX3 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]
  dX4 -> wX4 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]
  dX5 -> wX5 [color = black, penwidth = 1, arrowsize = 0.5, label = '', fontsize = 11]

  dY2 -> wY2 [color = black, penwidth = 1, arrowsize = 0.5]
  dY3 -> wY3 [color = black, penwidth = 1, arrowsize = 0.5]
  dY4 -> wY4 [color = black, penwidth = 1, arrowsize = 0.5]
  dY5 -> wY5 [color = black, penwidth = 1, arrowsize = 0.5]


  # # Within-time correlations - ONLY these are slightly rounded
  # wX1 -> wY1 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5 ]
  # wX2 -> wY2 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5 ]
  # wX3 -> wY3 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5 ]
  # wX4 -> wY4 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5 ]
  # wX5 -> wY5 [dir = both, color = black, style = dashed, splines = curved, constraint = false, arrowsize = 0.5 ]
  # 


  # Self-loops connected to bottom of each node using ports
  dX2 -> dX2 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s , arrowsize = 0.5]
  dX3 -> dX3 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.5]
  dX4 -> dX4 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.5]
  dX5 -> dX5 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s, arrowsize = 0.5]
  dY2 -> dY2 [dir = both, color = black, style = dashed, label = '', tailport = ne, headport = ne, arrowsize = 0.5]
  dY3 -> dY3 [dir = both, color = black, style = dashed, label = '', tailport = ne, headport = ne, arrowsize = 0.5]
  dY4 -> dY4 [dir = both, color = black, style = dashed, label = '', tailport = ne, headport = ne, arrowsize = 0.5]
  dY5 -> dY5 [dir = both, color = black, style = dashed, label = '', tailport = ne, headport = ne, arrowsize = 0.5]



  # Self-loops connected to bottom of each node using ports
  # X1 -> X1 [dir = both, color = black, style = dashed, label = '0', tailport = n, headport = n , arrowsize = 0.5, fontsize = 11]
  # X2 -> X2 [dir = both, color = black, style = dashed, label = '0', tailport = n, headport = n , arrowsize = 0.5, fontsize = 11]
  # X3 -> X3 [dir = both, color = black, style = dashed, label = '0', tailport = n, headport = n , arrowsize = 0.5, fontsize = 11]
  # X4 -> X4 [dir = both, color = black, style = dashed, label = '0', tailport = n, headport = n , arrowsize = 0.5, fontsize = 11]
  # X5 -> X5 [dir = both, color = black, style = dashed, label = '0', tailport = n, headport = n , arrowsize = 0.5, fontsize = 11]
  # 
  # Y1 -> Y1 [dir = both, color = black, style = dashed, label = '0', tailport = s, headport = s , arrowsize = 0.5, fontsize = 11]
  # Y2 -> Y2 [dir = both, color = black, style = dashed, label = '0', tailport = s, headport = s , arrowsize = 0.5, fontsize = 11]
  # Y3 -> Y3 [dir = both, color = black, style = dashed, label = '0', tailport = s, headport = s , arrowsize = 0.5, fontsize = 11]
  # Y4 -> Y4 [dir = both, color = black, style = dashed, label = '0', tailport = s, headport = s , arrowsize = 0.5, fontsize = 11]
  # Y5 -> Y5 [dir = both, color = black, style = dashed, label = '0', tailport = s, headport = s , arrowsize = 0.5, fontsize = 11]

  # Self-loops connected to bottom of each node using ports
   wX1 -> wX1 [dir = both, color = black, style = dashed, label = '', tailport = s, headport = s , arrowsize = 0.5, fontsize = 11, labelangle = 45, labeldistance = 2.5]
   wY1 -> wY1 [dir = both, color = black, style = dashed, label = '', tailport = n, headport = n , arrowsize = 0.5, fontsize = 11]

  # Residuals for random intercepts (epsilon)
  A_X -> A_X [dir = both, color = black, style = dashed, label = '', labelangle = 135, labeldistance = 1.5, tailport = s, headport = s, arrowsize = 0.5, width = 0.6, height = 0.3]
  A_Y -> A_Y [dir = both, color = black, style = dashed, label = '', labelangle = 135, labeldistance = 1.5, tailport = n, headport = n, arrowsize = 0.5, width = 0.6, height = 0.3]

}
")
lcm
```



```{r, echo = FALSE, message = FALSE, error = FALSE}
lcsm_data <- simLChange(
  waves = 10,
  variable_type = "bivariate",
  beta_x = -0.7,    # This should give CLPM beta ≈ 0.3
  beta_y = -0.7,    # This should give CLPM beta ≈ 0.3
  omega_x = 0.5,    # Cross-lagged coupling effects
  omega_y = 0.5,
  estimate_constant_change = TRUE, 
  latent_variance_x = 1,
  latent_variance_y = 1,
  sample.nobs = 10000
)
cat(paste("In this example, the DGP is a latent change model\n",
    "with constant effects. The estimator is a CLPM model,\n",
    "which doesn't fully control for unit effects.\n",
    "lavaan results are printed below"))
summary(lavaan(estimateCLPM(waves = 5), data = lcsm_data$data))
```

What about the RI-CLPM? If we simulate data under the RI-CLPM DGP, does the CLPM retrieve the correct parameters?

```{r}
#| echo: false
#| message: false
#| warning: false
ri <- simRICLPM(
  waves = 5,
  beta_x = 0.3,    
  beta_y = 0.3,   
  sample.nobs = 1000,
  var_BX = 0.5,
  var_BY = 0.5,
  var_p = 1,
  var_q = 1,
  estimate_means = TRUE
)
cat(paste("In this example, the DGP is the RICLPM\n",
    "The estimator is a CLPM model,\n",
    "which doesn't fully control for unit effects.\n",
    "lavaan results are printed below"))
summary(lavaan(estimateCLPM(waves = 5), data = ri$data))
```
Notice, the same problem exists. The model does a poor job retrieving the correct parameter estimates.

Why does the CLPM get it wrong? It only does as the proportion of the variance in $y$ (or $x$) increases due to unit effect -- unobserved heterogeneity. The CLPM assumes that all variance is wave-specific, and thus misattributes stable variance in $y$ to the autoregression parameters. This can be seen in examining the simulated trajectories of data generated varying the variance parameters for the random intercepts. First, let's consider a situation in which the **between** variance is 4 times as large as the within variance.

```{r trajectories2,  fig.cap="Individual trajectories under RICLPM"}
#| echo: false
#| message: false
#| warning: false
wide_data = simRICLPM(
  waves = 5,
  beta_x = 0.3,    
  beta_y = 0.8,    
  sample.nobs = 1000,
  var_BX = 2,
  var_BY = 2,
  var_p = 0.5,
  var_q = 0.5,
  estimate_means = TRUE)$data 

data = wide_data |>
  reshape_long_sim_cr() |>
  group_by(id) |>
  mutate(
    y_lag = lag(y),
    x_lag = lag(x),
    delta_y = y - lag(y)
  ) |>
  ungroup() -> data

data |>  
   ggplot(aes(x = wave, y = y, group = id)) + 
   geom_line(alpha = 0.1, color = "black", position = position_jitter(width = 0.1, height = 0.2)) +
   stat_summary(aes(group = 1), fun = mean, geom = "line",  
                color = "lightblue", size = 1, alpha = 0.4) + 
   stat_summary(aes(group = 1), fun = mean, geom = "point",  
                color = "lightblue", size = 1) + 
   labs(
     title = "Individual Trajectories: Simulated Data", 
     subtitle = "Between Variance = 4x as large as Within", 
     x = "Survey Wave",
     y = "Response" 
   ) +
  theme_minimal() + 
  scale_y_continuous(limits = c(-5, 6) ) + 
  theme(strip.text = element_text(color = "black")) -> plot 
print(plot)
```

This can be compared to the same DGP but where the **within** variance is 4 times as large as the **between** variance.

```{r trajectories3, fig.cap="Individual trajectories under RICLPM"}
library(dplyr)
library(ggplot2)
library(crossLagR)
library(lavaan)

wide_data = simRICLPM(
  waves = 5,
  beta_x = 0.3,    
  beta_y = 0.8,    
  sample.nobs = 1000,
  var_BX = 0.25,
  var_BY = 0.25,
  var_p = 1,
  var_q = 1,
  estimate_means = TRUE)$data 

data = wide_data |>
  reshape_long_sim_cr() |>
  group_by(id) |>
  mutate(
    y_lag = lag(y),
    x_lag = lag(x),
    delta_y = y - lag(y)
  ) |>
  ungroup() -> data

data |>  
   ggplot(aes(x = wave, y = y, group = id)) + 
   geom_line(alpha = 0.1, color = "black", position = position_jitter(width = 0.1, height = 0.2)) +
   stat_summary(aes(group = 1), fun = mean, geom = "line",  
                color = "lightblue", size = 1, alpha = 0.4) + 
   stat_summary(aes(group = 1), fun = mean, geom = "point",  
                color = "lightblue", size = 1) + 
   labs(
     title = "Individual Trajectories: Simulated Data", 
     subtitle = "Within Variance = 4x as large as Between", 
     x = "Survey Wave",
     y = "Response" 
   ) +
  theme_minimal() + 
  scale_y_continuous(limits = c(-5, 6) ) + 
  theme(strip.text = element_text(color = "black")) -> plot 
print(plot)
```

Figure \@ref(fig:trajectories2) shows real differences between units -- they're spread out; whereas Figure \@ref(fig:trajectories3) shows much less difference between units -- they're more tightly compact. The fundamental relationship between the points over time does not change, all that changes is the spread of the trajectories. 

## Model Validation

It may be useful to show these models can be estimated using techniques more familiar to a political science audience. Below, I estimate both the latent change model with constant effects and the RI-CLPM using `brms`.

### Hierarchical `brms` Estimation

This is the differenced variable regression with random intercepts. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
library(dplyr)
library(brms)
library(crossLagR)
# Simulate the data in this block
#####
simLChange(
  waves = 30,
  variable_type = "bivariate",
  beta_x = -0.7,    
  beta_y = -0.7,   
  indicator_variance_y = 1,
  indicator_variance_x = 1, 
  constant_mean_x = 0,
  constant_mean_y = 0 ,
  omega_x = 0.4,   
  omega_y = 0.4,
  estimate_constant_change = TRUE,  
  sample.nobs = 1000
)$data |>
# Here, I restructure to long format reshape_shape_cr is just a helper function to restructure the simulated data
  reshape_long_sim_cr() |>
  group_by(id) |>
    mutate(
      y_lag = lag(y),
      x_lag = lag(x),
      delta_y = y - lag(y),
      delta_x = x - lag(x)
  ) |>
  ungroup() -> data
######
##
model = brm(mvbind(delta_y, delta_x) ~ -1 +y_lag + 
              x_lag + (1|id),
    iter = 1000, ### The random intercept model in brms is the same structure as lme4, and nested formulas in R
    cores = 10,
    chains = 3,
    data = data, 
    family= gaussian())
```

```{r}
summary(model)
```


## Short *t* bias. 

It is well established that estimating a dynamic fixed effects model with short *t* -- relatively few waves -- can lead to biased estimates. This is shown in @hsiao2022. To see this, here is a dynamic panel data simulation where the DGP is an RI-CLPM with known parameters. 

## The Fixed Effect (Dynamic Panel) Estimator

This example simulates data from a four wave panel. Notice that the parameter estimates are far from their true values.

```{r}
#| echo: true
#| message: false
#| warning: false
library(crossLagR)
library(dplyr)
library(purrr)
library(tidyr)

# Long data
baseRICLPM_sim(n = 1000,
               ar_x = 0.5,
               ar_y = 0.5,
               cl_xy = 0.3,
               cl_yx = 0.2, 
               waves = 4) |>
  select(-contains("trait")) |>
  # reshape to long
  reshape_long_sim_cr() -> dat
lm(within.y ~  xlagw + ylagw, data = dat) |> 
  summary()
```

With more waves $t = 30$, the model recovers the true parameters.

```{r}

#| echo: true
#| message: false
#| warning: false
# Long data
library(crossLagR)
library(dplyr)
library(purrr)
library(tidyr)


baseRICLPM_sim(n = 1000,
               ar_x = 0.5,
               ar_y = 0.5,
               cl_xy = 0.3,
               cl_yx = 0.2, 
               waves = 30) |>
  select(-contains("trait")) |>
  # reshape to long
  reshape_long_sim_cr() -> dat
lm(within.y ~  xlagw + ylagw, data = dat) |> 
  summary()
```

## The RICLPM

The RICLPM doesn't exhibit the same problem, with minimal bias (that decreases) with more waves. The code below runs a simple
Monte Carlo analysis varying the number of panel waves, and the following fixed parameters ($n = 2000; \beta_x = 0.4; \beta_y = 0.7; \omega_{x \rightarrow y} = 0.2; \omega_{y \rightarrow x} = 0.3$). 

```{r}
#| echo: true
#| message: false
#| warning: false
library(crossLagR)
library(dplyr)
library(purrr)
library(ggplot2)
library(tidyr)

true_params <- list(beta_x = 0.4, beta_y = 0.7, 
                    omega_xy = 0.2, omega_yx = 0.3)

extract_params <- function(fit) {
  pe <- lavaan::parameterEstimates(fit)
  factor_scores <- lavaan::lavPredict(fit, method = "EBM")
  factor_scores_df <- as_tibble(factor_scores) %>%
      select(starts_with("general_y") | starts_with("general_x"))
  tibble(
    beta_x = pe$est[pe$label ==   "beta_x"][1],
    beta_y = pe$est[pe$label ==   "beta_y"][1],
    omega_xy = pe$est[pe$label == "omega_xy"][1],
    omega_yx = pe$est[pe$label == "omega_yx"][1],
  )
}


results <- expand_grid(
  waves = c(3, 5, 7, 10, 30),
  rep = 1:50
) |>
  mutate(
    data = map(waves, ~{
      baseRICLPM_sim(
        n = 2000,
        innov_var = 0.45,
        ar_x = 0.4,  ar_y = 0.7,
        cl_xy = 0.3, cl_yx = 0.2,
        waves = .x
      ) |>
        select(-contains("trait"))
    }),
    # This iterates over the data, x and waves y
    fit = map2(data, waves, ~{
      lavaan::lavaan(estimateRICLPM(waves = ..2,
                   label_autoregressive = c("beta_x", "beta_y")), data = ..1, 
                     warn = FALSE, verbose = FALSE)
    }),
    params = map(fit, extract_params)
  ) |>
  unnest(params)

```

```{r}
library(ggridges)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggridges)  

# Prepare data with simpler labels for parsing
results_long <- results |>
  select(waves, rep, beta_x, beta_y, omega_xy, omega_yx) |>
  pivot_longer(cols = c(beta_x, beta_y, omega_xy, omega_yx),
               names_to = "parameter", 
               values_to = "estimate") |>
  mutate(
    parameter_label = case_when(
      parameter == "beta_x" ~ "Autoregressive[x]",
      parameter == "beta_y" ~ "Autoregressive[y]",
      parameter == "omega_xy" ~ "CrossLag~X%->%Y~(omega[xy])",
      parameter == "omega_yx" ~ "CrossLag~Y%->%X~(omega[yx])"
    ),
    true_value = case_when(
      parameter == "beta_x" ~ 0.4,
      parameter == "beta_y" ~ 0.7,
      parameter == "omega_xy" ~ 0.2,
      parameter == "omega_yx" ~ 0.3
    ),
    waves_factor = factor(waves, levels = c(30, 10, 7, 5, 3))
  )

# Calculate medians for each wave/parameter combination
medians <- results_long |>
  group_by(waves_factor, parameter_label, true_value) |>
  summarise(median_est = median(estimate), .groups = "drop")

# Create plot with median lines
ggplot(results_long, aes(x = estimate, y = waves_factor)) +
  geom_density_ridges(
    aes(fill = waves_factor),
    scale = 2, 
    rel_min_height = 0.01,
    alpha = 0.7
  ) +
  geom_vline(aes(xintercept = true_value), 
             linetype = "dashed", color = "black", size = 0.8, alpha = 0.5) +
  geom_segment(data = medians,
               aes(x = median_est, xend = median_est,
                   y = as.numeric(waves_factor) - 0.3, 
                   yend = as.numeric(waves_factor) + 0.3),
               color = "green", size = 1) +
  facet_wrap(~parameter_label, scales = "free_x", 
             labeller = label_parsed, nrow = 2) +
  scale_fill_viridis_d(option = "viridis", begin = 0.2, end = 0.9) +
  labs(
    title = "Monte Carlo Sample Distribution Varying Waves, RI-CLPM",
    subtitle = "Green line = median estimate | Dashed line = true value",
    x = "Parameter Estimate",
    y = "Number of Waves"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "none",
    strip.text = element_text(size = 11, face = "bold"),
    panel.grid.minor = element_blank()
  )
```

Here is a summary table,

```{r}
library(gt)
bias_summary <- results |>
  group_by(waves) |>
  summarise(
    across(c(beta_x, beta_y, omega_xy, omega_yx),
           list(
             mean = mean,
             bias = ~mean(.) - true_params[[cur_column()]],
             relative_bias = ~ (mean(.) - true_params[[cur_column()]]) / true_params[[cur_column()]]*100,
             se = ~sd(.)
           ),
           .names = "{.col}_{.fn}")
  ) |>
  select(waves,
         contains("bias")) |>
  mutate(across(where(is.numeric), ~round(., 3)))

bias_summary |>
  gt() |>
  tab_header(
    title = md("**Monte Carlo Bias Analysis**"),
    subtitle = "RICLPM Estimation: Varying the Number of Waves"
  ) |>
  cols_label(
    waves = "Waves",
    beta_x_bias =  "Bias, AR(X)",
    beta_y_bias =  "Bias, AR(Y)",
    beta_x_relative_bias  =  "Relative Bias AR(X) %",
    beta_y_relative_bias  =  "Relative Bias AR(Y) %",
    omega_xy_bias  =  "Bias, CL(X→Y)",
    omega_yx_bias  =  "Bias, CL(Y→X)",
    omega_xy_relative_bias  =  "Relative Bias, CL(X→Y) %",
    omega_yx_relative_bias  =  "Relative Bias, CL(Y→X) %"
)|>
   tab_source_note(
     source_note = md("*Note*: The results reported here are from 100 simulations, varying the number of waves.")
    ) |>
    tab_footnote(
      footnote = "Bias = Mean(estimate) - True value, Relative Bias = (Bias / True value) * 100")
```
## Change

Below I attempt to do the same analysis but with the latent change score model with constant effects. In these simulations, I vary the degree of "between-unit" and "within-wave" variation. I didn't have much luck here....strange results.


```{r}
#| echo: true
#| message: false
#| warning: false
library(dplyr)
library(purrr)
library(tidyr)

baseRICLPM_sim(n = 1000,
               ar_x = 0.4,
               ar_y = 0.7,
               cl_xy = 0.3,
               cl_yx = 0.2, 
               waves = 10) |>
  select(-contains("trait")) -> dat

fit = lavaan::lavaan(estimateLChange(waves = 10,
                                  variable_type =    "bivariate"), 
                                 data = dat, 
                     warn = FALSE, verbose = FALSE)


true_params <- list(beta_x = -0.6 , beta_y = -0.3, 
                    omega_x = 0.2, omega_y = 0.3)

extract_params <- function(fit) {
  pe <- lavaan::parameterEstimates(fit)
  tibble(
    beta_x = pe$est[pe$label ==   "beta_x"][1],
    beta_y = pe$est[pe$label ==   "beta_y"][1],
    omega_x = pe$est[pe$label == "omega_x"][1],
    omega_y = pe$est[pe$label == "omega_y"][1]
  )
}

results <- expand_grid(
  waves = c(3, 5, 10, 30),
  innov_var = seq(0.25, 1.25, by = 1),
  rep = 1:50
) |>
  mutate(
  data = pmap(list(waves = waves, innov_var = innov_var), ~{
  baseRICLPM_sim(
    n = 2000,
    innov_var = ..2,  # second arg pmap
    ar_x = 0.4, ar_y = 0.7,
    cl_xy = 0.3, cl_yx = 0.2,
    waves = ..1 # first arg in pmap
  )
}),
    # This iterates over the data, x and waves y
    fit = map2(data, waves, ~{
      lavaan::lavaan(estimateLChange(waves = ..2,
                                     variable_type = "bivariate",
                                     estimate_constant_change = FALSE), 
                                 data = ..1, 
                     warn = FALSE, verbose = FALSE)
    }),
    params = map(fit, extract_params)
  ) |>
  unnest(params)

results |>
  mutate(
    beta_y = beta_y ,
    beta_x = beta_x ) |>
  group_by(waves, innov_var) |>
  summarise(
    across(c(beta_x, beta_y, omega_y, omega_x),
           list(
             mean = mean,
             bias = ~mean(.) - true_params[[cur_column()]],
             relative_bias = ~ (mean(.) - true_params[[cur_column()]]) / true_params[[cur_column()]]*100,
             se = ~sd(.)
           ),
           .names = "{.col}_{.fn}")) -> bias_summary

```


## Change

```{r}
library(gt)
bias_summary |>
  select(waves,
         contains(c("mean", "waves", "innov_var"))) |>
  mutate(across(where(is.numeric), ~round(., 3))) 
```


## Summary

The change (LCM) and level score (CLPM; RICLPM) analysis are equivalent in the absence of unit effects. This suggests a close relationship between these models. However, when unit effects are present, the models diverge. When the DGP includes unit effects, the same interchangeable relationships disappear.

